{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761ff86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import copy\n",
    "train_data=pd.read_excel(r\"E:\\pxdata\\use\\5_5no1.xlsx\",engine='openpyxl')\n",
    "train_label= pd.read_excel(r\"E:\\pxdata\\use\\weizhi1.xlsx\",engine='openpyxl')\n",
    "#test_data=pd.read_excel(r\"C:\\Users\\zhaoyuan\\Desktop\\数学建模\\赛题\\D\\原始数据\\Molecular_Descriptor.xlsx\",engine='openpyxl',sheet_name='test')\n",
    "#test_label= pd.read_excel(r\"C:\\Users\\zhaoyuan\\Desktop\\数学建模\\赛题\\D\\原始数据\\ERα_activity.xlsx\",engine='openpyxl',sheet_name='test')\n",
    "train_data\n",
    "#train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ed4ee1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x   y\n",
       "0    5   5\n",
       "1    5   5\n",
       "2    5   5\n",
       "3    5   5\n",
       "4    5   5\n",
       "5    5   5\n",
       "6    5   5\n",
       "7    5   5\n",
       "8    5   5\n",
       "9    5   5\n",
       "10   5   5\n",
       "11   5   5\n",
       "12   5   5\n",
       "13  10  15\n",
       "14  10  15\n",
       "15  10  15\n",
       "16  10  15\n",
       "17  10  15\n",
       "18  10  20\n",
       "19  10  20\n",
       "20  10  20\n",
       "21  10  20\n",
       "22  10  20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c65cbce7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-1966bc991de9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'figure.figsize'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m12.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'font.sans-serif'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'SimHei'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"分布图\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"imgs/label_fig.png\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4295\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4296\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4297\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4299\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12.0, 12.0)\n",
    "plt.rcParams['font.sans-serif'] = 'SimHei'\n",
    "plt.title(train_label.columns[2]+\"分布图\")\n",
    "sns.distplot(train_label.iloc[:,2])\n",
    "plt.savefig(\"imgs/label_fig.png\",dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675aea0a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     False\n",
       "1     False\n",
       "2     False\n",
       "3     False\n",
       "4     False\n",
       "5     False\n",
       "6     False\n",
       "7     False\n",
       "8     False\n",
       "9     False\n",
       "10    False\n",
       "11    False\n",
       "12    False\n",
       "13    False\n",
       "14    False\n",
       "15    False\n",
       "16    False\n",
       "17    False\n",
       "18    False\n",
       "19    False\n",
       "20    False\n",
       "21    False\n",
       "22    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.iloc[:,0]==train_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c501dc9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>通道1到达时间ns</th>\n",
       "      <th>通道1幅度dB</th>\n",
       "      <th>通道1振铃计数</th>\n",
       "      <th>通道1持续时间us</th>\n",
       "      <th>通道1能量</th>\n",
       "      <th>通道2到达时间ns</th>\n",
       "      <th>通道2幅度dB</th>\n",
       "      <th>通道2振铃计数</th>\n",
       "      <th>通道2持续时间us</th>\n",
       "      <th>通道2能量</th>\n",
       "      <th>通道3到达时间ns</th>\n",
       "      <th>通道3幅度dB</th>\n",
       "      <th>通道3振铃计数</th>\n",
       "      <th>通道3持续时间us</th>\n",
       "      <th>通道3能量</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [通道1到达时间ns, 通道1幅度dB, 通道1振铃计数, 通道1持续时间us, 通道1能量, 通道2到达时间ns, 通道2幅度dB, 通道2振铃计数, 通道2持续时间us, 通道2能量, 通道3到达时间ns, 通道3幅度dB, 通道3振铃计数, 通道3持续时间us, 通道3能量]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[pd.isnull(train_data).T.any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "229c53e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#一列只有一个数值的变量 全为一个值\n",
    "class_col_1 = []\n",
    "#一列只有多个数值的变量\n",
    "class_col = []\n",
    "# class_col_2 = []\n",
    "for col in range(train_data.shape[1]):\n",
    "    num=0\n",
    "    vals = set()\n",
    "    for row in range(train_data.shape[0]):\n",
    "        vals.add(train_data.iloc[row,col])\n",
    "        if len(vals)>2:break\n",
    "    if len(vals)==1:class_col_1.append(col)\n",
    "    else: class_col.append(col)\n",
    "#     else \n",
    "class_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3f7775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(class_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "475bc5b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# float64 = []\n",
    "# int64 = []\n",
    "# int64_num = []\n",
    "# class_col_2 = []\n",
    "#判断是否像[0,1,2]类型变量\n",
    "def judge_int64(myset):\n",
    "    last = -1\n",
    "    for x in myset:\n",
    "        if x-last!=1:\n",
    "            return False\n",
    "        last = x\n",
    "    return True\n",
    "#判断是否是离散或者连续变量\n",
    "def get_int_float(mydata):\n",
    "    myfloat64 = []\n",
    "    myint64 = []\n",
    "#     myint64_num = []\n",
    "    for col in range(1,mydata.shape[1]):\n",
    "        flag = True\n",
    "        vals = set()\n",
    "        for row in range(mydata.shape[0]):\n",
    "            val=mydata.iloc[row,col]\n",
    "            vals.add(val)\n",
    "            if type(val)==np.float64:\n",
    "                flag = False\n",
    "                break\n",
    "        #判断值是否连续   \n",
    "        if flag and len(vals)<10 and judge_int64(vals):\n",
    "            myint64.append(col)\n",
    "         \n",
    "        #         myint64_num.append(len(vals))\n",
    "        else: myfloat64.append(col)\n",
    "    return myint64,myfloat64\n",
    "int64,float64 = get_int_float(train_data)\n",
    "#     else \n",
    "len(int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7763c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异常值处理都需要对连续值进行\n",
    "float_train_data = train_data.iloc[:,float64]\n",
    "min_range= float_train_data.mean()-3*float_train_data.std()\n",
    "max_range= float_train_data.mean()+3*float_train_data.std()\n",
    "cols_mean = float_train_data.mean()\n",
    "error_index = []\n",
    "for row in range(float_train_data.shape[0]):\n",
    "    for col in range(float_train_data.shape[1]):\n",
    "        if float_train_data.iloc[row,col]<min_range[col] or float_train_data.iloc[row,col]>max_range[col]:\n",
    "            error_index.append([row,col])\n",
    "            float_train_data.iloc[row,col]=cols_mean[col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a199364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b878e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.iloc[:,float64] = float_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "842967b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>通道1幅度dB</th>\n",
       "      <th>通道1振铃计数</th>\n",
       "      <th>通道1持续时间us</th>\n",
       "      <th>通道1能量</th>\n",
       "      <th>通道2到达时间ns</th>\n",
       "      <th>通道2幅度dB</th>\n",
       "      <th>通道2振铃计数</th>\n",
       "      <th>通道2持续时间us</th>\n",
       "      <th>通道2能量</th>\n",
       "      <th>通道3到达时间ns</th>\n",
       "      <th>通道3幅度dB</th>\n",
       "      <th>通道3振铃计数</th>\n",
       "      <th>通道3持续时间us</th>\n",
       "      <th>通道3能量</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>通道1幅度dB</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.751007</td>\n",
       "      <td>0.522150</td>\n",
       "      <td>0.823012</td>\n",
       "      <td>0.120896</td>\n",
       "      <td>0.623733</td>\n",
       "      <td>0.025522</td>\n",
       "      <td>0.004661</td>\n",
       "      <td>-0.087166</td>\n",
       "      <td>0.120897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.175961</td>\n",
       "      <td>0.313672</td>\n",
       "      <td>0.519770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>通道1振铃计数</th>\n",
       "      <td>0.751007</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.859958</td>\n",
       "      <td>0.649180</td>\n",
       "      <td>0.155450</td>\n",
       "      <td>0.319197</td>\n",
       "      <td>-0.377738</td>\n",
       "      <td>-0.420223</td>\n",
       "      <td>-0.500821</td>\n",
       "      <td>0.155451</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.360482</td>\n",
       "      <td>0.364418</td>\n",
       "      <td>0.787839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>通道1持续时间us</th>\n",
       "      <td>0.522150</td>\n",
       "      <td>0.859958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.306651</td>\n",
       "      <td>0.306940</td>\n",
       "      <td>-0.098304</td>\n",
       "      <td>-0.713389</td>\n",
       "      <td>-0.765177</td>\n",
       "      <td>-0.852229</td>\n",
       "      <td>0.306941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.533134</td>\n",
       "      <td>0.459980</td>\n",
       "      <td>0.906036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>通道1能量</th>\n",
       "      <td>0.823012</td>\n",
       "      <td>0.649180</td>\n",
       "      <td>0.306651</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.135415</td>\n",
       "      <td>0.734006</td>\n",
       "      <td>0.130522</td>\n",
       "      <td>0.177810</td>\n",
       "      <td>0.175155</td>\n",
       "      <td>0.135417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.068477</td>\n",
       "      <td>0.060753</td>\n",
       "      <td>0.413992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>通道2到达时间ns</th>\n",
       "      <td>0.120896</td>\n",
       "      <td>0.155450</td>\n",
       "      <td>0.306940</td>\n",
       "      <td>0.135415</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.061738</td>\n",
       "      <td>-0.133478</td>\n",
       "      <td>-0.321018</td>\n",
       "      <td>-0.304665</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.420840</td>\n",
       "      <td>0.397505</td>\n",
       "      <td>0.287629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            通道1幅度dB   通道1振铃计数  通道1持续时间us     通道1能量  通道2到达时间ns   通道2幅度dB  \\\n",
       "通道1幅度dB    1.000000  0.751007   0.522150  0.823012   0.120896  0.623733   \n",
       "通道1振铃计数    0.751007  1.000000   0.859958  0.649180   0.155450  0.319197   \n",
       "通道1持续时间us  0.522150  0.859958   1.000000  0.306651   0.306940 -0.098304   \n",
       "通道1能量      0.823012  0.649180   0.306651  1.000000   0.135415  0.734006   \n",
       "通道2到达时间ns  0.120896  0.155450   0.306940  0.135415   1.000000 -0.061738   \n",
       "\n",
       "            通道2振铃计数  通道2持续时间us     通道2能量  通道3到达时间ns  通道3幅度dB   通道3振铃计数  \\\n",
       "通道1幅度dB    0.025522   0.004661 -0.087166   0.120897      NaN -0.175961   \n",
       "通道1振铃计数   -0.377738  -0.420223 -0.500821   0.155451      NaN -0.360482   \n",
       "通道1持续时间us -0.713389  -0.765177 -0.852229   0.306941      NaN -0.533134   \n",
       "通道1能量      0.130522   0.177810  0.175155   0.135417      NaN  0.068477   \n",
       "通道2到达时间ns -0.133478  -0.321018 -0.304665   1.000000      NaN -0.420840   \n",
       "\n",
       "           通道3持续时间us     通道3能量  \n",
       "通道1幅度dB     0.313672  0.519770  \n",
       "通道1振铃计数     0.364418  0.787839  \n",
       "通道1持续时间us   0.459980  0.906036  \n",
       "通道1能量       0.060753  0.413992  \n",
       "通道2到达时间ns   0.397505  0.287629  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_limit = 0.8#0.8以上认为是强相关 \n",
    "corr_matrix = train_data.iloc[:,1:].corr(\"pearson\") \n",
    "corr_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46718e2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['通道1能量'],\n",
       " ['通道1持续时间us'],\n",
       " ['通道3能量'],\n",
       " [],\n",
       " ['通道3到达时间ns'],\n",
       " [],\n",
       " ['通道2持续时间us', '通道2能量'],\n",
       " ['通道2能量'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "corr_index = []\n",
    "corr_num = []\n",
    "corr_names = []\n",
    "for row in range(corr_matrix.shape[0]):\n",
    "    num=0\n",
    "    names = []\n",
    "    indexs = []\n",
    "    for col in range(corr_matrix.shape[1]):\n",
    "        if corr_matrix.iloc[row,col]>corr_limit and row!=col and col>row:\n",
    "            indexs.append(corr_matrix.columns[col])\n",
    "            \n",
    "        if corr_matrix.iloc[row,col]>corr_limit:\n",
    "            names.append(corr_matrix.columns[col])\n",
    "            num+=1\n",
    "    corr_num.append(num)\n",
    "    corr_names.append(names)\n",
    "    corr_index.append(indexs)\n",
    "corr_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35a8d978",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['通道1幅度dB',\n",
       " '通道1振铃计数',\n",
       " '通道1持续时间us',\n",
       " '通道1能量',\n",
       " '通道2到达时间ns',\n",
       " '通道2幅度dB',\n",
       " '通道2振铃计数',\n",
       " '通道2持续时间us',\n",
       " '通道2能量',\n",
       " '通道3到达时间ns',\n",
       " '通道3幅度dB',\n",
       " '通道3振铃计数',\n",
       " '通道3持续时间us',\n",
       " '通道3能量']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 即将删掉的列，强相关的自变量仅保留其一\n",
    "del_cols = []\n",
    "bianli_cols = []\n",
    "arg_indexs = (-np.array(corr_num)).argsort()\n",
    "for row in range(len(corr_index)):\n",
    "    bianli_cols.append(corr_matrix.columns[row])\n",
    "    for col in corr_index[row]:\n",
    "        if col not in del_cols and col not in bianli_cols:\n",
    "            \n",
    "            、、del_cols.append(col)\n",
    "del_cols\n",
    "bianli_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ea0e2d1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-11f3cec8c78d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorr_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marg_indexs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mgrr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'o'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhist_kwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'bins'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#,cmap=mglearn.cm3)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"imgs/第1题强相关变量之间的散点图1.png\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    887\u001b[0m                     \u001b[1;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m             \u001b[1;31m# we by definition only have the 0th axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1448\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1450\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1451\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIndexingError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1452\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 723\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1356\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1359\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m             \u001b[1;31m# a tuple should already have been caught by this point\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1442\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1444\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m     \u001b[1;31m# -------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "\n",
    "import copy \n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14.0, 14.0)\n",
    "temp = copy.dee pcopy(corr_names[arg_indexs[0]])\n",
    "temp = temp[len(temp)-5:-1]\n",
    "temp.append(corr_matrix.columns[arg_indexs[0]])\n",
    "grr = pd.plotting.scatter_matrix(train_data[temp],marker='o',c = train_label.iloc[:,2],hist_kwds={'bins':20})#,cmap=mglearn.cm3)\n",
    "plt.savefig(\"imgs/第1题强相关变量之间的散点图1.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa5e7295",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-83e3608c4700>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mint64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloat64\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_int_float\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mori_train_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mori_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mori_train_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mori_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mori_train_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data.iloc[:,float64] = (train_data.iloc[:,float64]-train_data.iloc[:,float64].mean())/train_data.iloc[:,float64].std()\n",
    "int64,float64 = get_int_float(ori_train_data)\n",
    "\n",
    "ori_std = ori_train_data.std()\n",
    "ori_mean = ori_train_data.mean()\n",
    "#毒性方向 使1为最好值\n",
    "train_label.iloc[:,[4,6]] = 1-train_label.iloc[:,[4,6]]\n",
    "ori_train_data = (ori_train_data-ori_train_data.mean())/ori_train_data.std()\n",
    "ori_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01851a4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ec31083ad53a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import trunc_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5defaf3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intercept_: 9.050263319913622\n",
      "coef_: [-4.60388955e-05  1.19673734e-01 -1.24400245e-02 -2.80367391e-05\n",
      " -1.42932311e-04  1.03081316e-04 -3.09395997e-02  5.19348561e-03\n",
      "  1.33521248e-05 -5.27532115e-05 -5.70424401e-05 -7.11236625e-16\n",
      "  6.20300835e-05  4.20094024e-05  6.72220045e-05]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "#导入模型，模型参数默认\n",
    "LR = linear_model.LinearRegression()\n",
    "#训练模型\n",
    "LR.fit(train_data.iloc[:,:],train_label.iloc[:,1])\n",
    "#预测模型LR.predict(X_test),此时输出类别数据\n",
    "#打印截距\n",
    "print('intercept_:' ,LR.intercept_)\n",
    "#打印模型系数\n",
    "print('coef_:', LR.coef_)\n",
    "#LRcoef = train_data.iloc[:,1:].columns[(-abs(LR.coef_)).argsort()[:var_num]]\n",
    "#LRcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e147458e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'BPNN_DATA_Reg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-701372cb3eec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# 生成训练数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mBPNN_DATA_Reg\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#  输入数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mxdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'BPNN_DATA_Reg'"
     ]
    }
   ],
   "source": [
    "\n",
    "# -*- coding：utf-8 -*-\n",
    "# &Author  AnFany\n",
    "\n",
    "# 利用神经网络拟合函数\n",
    "\n",
    "# 生成训练数据\n",
    "import numpy as np\n",
    "import BPNN_DATA_Reg as bp\n",
    "#  输入数据\n",
    "xdata = train_data.iloc[:,:]\n",
    "\n",
    "#  输出数据\n",
    "#y1data = train_label.iloc[:,0]\n",
    "#y2data = train_label.iloc[:,1]\n",
    "\n",
    "#  两个输出数据和二为一，\n",
    "ydata = train_label.iloc[:,:]\n",
    "\n",
    "# 数据标准化\n",
    "xdata = (xdata - np.mean(xdata, axis=0)) / np.std(xdata, axis=0)\n",
    "\n",
    "# 最大与最小值\n",
    "maxnum = np.max(ydata, axis=0, keepdims=True)\n",
    "\n",
    "minnum = np.min(ydata, axis=0, keepdims=True)\n",
    "\n",
    "nor m_ydata = (ydata - minnum) / (maxnum - minnum)\n",
    "\n",
    "# 分为训练数据和测试数据\n",
    "model_data = bp.divided(xdata, norm_ydata, percent=0.1)\n",
    "\n",
    "# 训练的输入、输出\n",
    "train_x_in = model_data[0]\n",
    "train_y_out = model_data[1]\n",
    "\n",
    "# 预测的输入、输出\n",
    "pre_x_in = model_data[2]\n",
    "pre_y_out = model_data[3]\n",
    "\n",
    "\n",
    "# 引入AnFany以及TensorFlow方法\n",
    "\n",
    "import AnFany_BPNN_Regression as An_Bpnn  \n",
    "import TensorFlow_BPNN_Regression as Ten_Bpnn  \n",
    "\n",
    "\n",
    "# AnFany方法\n",
    "# # 开始训练数据\n",
    "bpnn = An_Bpnn.BPNN(train_x_in, train_y_out, learn_rate=0.002, son_samples=50, iter_times=100000000, \\\n",
    "                    hidden_layer=[190, 190, 190], break_error=0.005)\n",
    "bpnn_train = bpnn.train_adam()\n",
    "\n",
    "# 训练结果展示\n",
    "train_output = An_Bpnn.trans(bpnn.predict(train_x_in), minnum, maxnum)\n",
    "An_Bpnn.figure(An_Bpnn.trans(train_y_out, minnum, maxnum), train_output, le='训练', width=4)\n",
    "\n",
    "pre_output = An_Bpnn.trans(bpnn.predict(pre_x_in), minnum, maxnum)\n",
    "An_Bpnn.figure(An_Bpnn.trans(pre_y_out, minnum, maxnum), pre_output, le='预测', width=2)\n",
    "\n",
    "An_Bpnn.costfig(bpnn_train[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a19cafd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'timm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ec31083ad53a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'timm'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ca0c640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-74815025b087>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[0mfile_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'E:\\pxdata\\use\\data_t1.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     \u001b[0mdata_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlab_train_onehot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlab_test_onehot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdic_index2str\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mload_dataset_iris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-74815025b087>\u001b[0m in \u001b[0;36mload_dataset_iris\u001b[1;34m(file_data, N_train)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_dataset_iris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;31m# 数据读取\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m     \u001b[0mdatas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[0mlabs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[1;31m# converting the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1146\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1147\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1148\u001b[0m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[1;34m(chunk_size)\u001b[0m\n\u001b[0;32m    988\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m                 \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m                 \u001b[0mline_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    988\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m                 \u001b[0mvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m                 \u001b[0mline_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmod(z):\n",
    "    h = 1./(1+np.exp(-z))\n",
    "    return h\n",
    "    \n",
    "def de_sigmoid(z,h):\n",
    "    return h*(1-h)\n",
    "    \n",
    "    \n",
    "def relu(z):\n",
    "    h = np.maximum(z, 0)\n",
    "    return h\n",
    "    \n",
    "def de_relu(z,h):\n",
    "    z[z <= 0] = 0\n",
    "    z[z > 0] = 1.0\n",
    "    return z\n",
    "    \n",
    "    \n",
    "    \n",
    "def no_active(z):\n",
    "    h = z\n",
    "    return h\n",
    "\n",
    "def de_no_active(z,h):\n",
    "    return np.ones(h.shape)\n",
    "    \n",
    "# o Nxc\n",
    "# lab Nxc    \n",
    "def loss_L2(o,lab):\n",
    "    diff = lab-o\n",
    "    sqrDiff = diff ** 2\n",
    "    return 0.5*np.sum(sqrDiff)\n",
    "    \n",
    "def de_loss_L2(o,lab):\n",
    "    return o-lab\n",
    "\n",
    "\n",
    "def loss_CE(o,lab):    \n",
    "    p = np.exp(o)/np.sum(np.exp(o),axis=1,keepdims=True)\n",
    "    loss_ce = np.sum(-lab*np.log(p))\n",
    "    return loss_ce\n",
    "\n",
    "def de_loss_CE(o,lab):\n",
    "    p = np.exp(o)/np.sum(np.exp(o),axis=1,keepdims=True)\n",
    "    return p-lab\n",
    "\n",
    "# dim_in:输入特征的维度\n",
    "# list_num_hidden： 每层输出节点的数目\n",
    "# list_act_funs： 每层的激活函数\n",
    "# list_de_act_funs: 反向传播时的函数\n",
    "\n",
    "def bulid_net(dim_in,list_num_hidden,\n",
    "              list_act_funs,list_de_act_funs):\n",
    "    layers=[]          \n",
    "    \n",
    "    # 逐层的进行网络构建\n",
    "    for i in range(len(list_num_hidden)):\n",
    "        layer = {}\n",
    "        \n",
    "        # 定义每一层的权重\n",
    "        if i ==0:\n",
    "            # layer[\"w\"]= 0.2*np.random.randn(dim_in,list_num_hidden[i])-0.1 # 用sigmoid激活函数\n",
    "            layer[\"w\"]= 0.01*np.random.randn(dim_in,list_num_hidden[i])  # 用relu 激活函数\n",
    "        else:\n",
    "            # layer[\"w\"]= 0.2*np.random.randn(list_num_hidden[i-1],list_num_hidden[i])-0.1 # 用sigmoid激活函数\n",
    "            layer[\"w\"]= 0.01*np.random.randn(list_num_hidden[i-1],list_num_hidden[i]) # 用relu 激活函数\n",
    "        \n",
    "        # 定义每一层的偏置\n",
    "        layer[\"b\"] = 0.1*np.ones([1,list_num_hidden[i]])\n",
    "        layer[\"act_fun\"]= list_act_funs[i]\n",
    "        layer[\"de_act_fun\"]= list_de_act_funs[i]\n",
    "        layers.append(layer)\n",
    "        \n",
    "    return layers\n",
    "    \n",
    "    \n",
    "# 返回每一层的输入\n",
    "# 与最后一层的输出    \n",
    "def fead_forward(datas,layers):\n",
    "    input_layers = []\n",
    "    input_acfun = []\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if i ==0:\n",
    "            inputs = datas\n",
    "            z = np.dot(inputs,layer[\"w\"]) + layer[\"b\"]\n",
    "            h = layer['act_fun'](z)\n",
    "            input_layers.append(inputs)\n",
    "            input_acfun.append(z)\n",
    "        else:\n",
    "            inputs = h\n",
    "            z = np.dot(inputs,layer[\"w\"])+ layer[\"b\"]\n",
    "            h = layer['act_fun'](z)\n",
    "            input_layers.append(inputs)\n",
    "            input_acfun.append(z)\n",
    "    return input_layers,input_acfun,h\n",
    "\n",
    "\n",
    "# 进行参数更新更新    \n",
    "def updata_wb(datas,labs,layers, loss_fun,de_loss_fun,alpha=0.01):\n",
    "    N,D = np.shape(datas)\n",
    "    # 进行前馈操作\n",
    "    inputs,input_acfun,output = fead_forward(datas,layers)\n",
    "    # 计算 loss\n",
    "    loss = loss_fun(output,labs)\n",
    "    #从后向前计算\n",
    "    deltas0 = de_loss_fun(output,labs)\n",
    "    # 从后向前计算误差\n",
    "    deltas =[]\n",
    "    for i in range(len(layers)):\n",
    "        index = -i-1\n",
    "        if i ==0:\n",
    "            h = output\n",
    "            z = input_acfun[index]\n",
    "            delta = deltas0*layers[index][\"de_act_fun\"](z,h)\n",
    "        else:\n",
    "            h = inputs[index+1]\n",
    "            z = input_acfun[index]\n",
    "            # print(layers[index][\"de_act_fun\"](z,h)[1])\n",
    "            delta = np.dot(delta,layers[index+1][\"w\"].T)*layers[index][\"de_act_fun\"](z,h)\n",
    "        \n",
    "        deltas.insert(0,delta)\n",
    "    \n",
    "    # 利用误差 对每一层的权重进行修成\n",
    "    for i in range(len(layers)):\n",
    "        # 计算 dw 与 db\n",
    "        dw = np.dot(inputs[i].T,deltas[i])\n",
    "        db = np.sum(deltas[i],axis=0,keepdims=True)\n",
    "        # 梯度下降\n",
    "        layers[i][\"w\"] = layers[i][\"w\"] - alpha*dw\n",
    "        layers[i][\"b\"] = layers[i][\"b\"] - alpha*db\n",
    "        \n",
    "    return layers,loss\n",
    "    \n",
    "    \n",
    "def test_accuracy(datas,labs_true,layers):\n",
    "    _,_,output = fead_forward(datas,layers)\n",
    "    lab_det = np.argmax(output,axis=1)\n",
    "    labs_true = np.argmax(labs_true,axis=1) \n",
    "    N_error = np.where(np.abs(labs_true-lab_det)>0)[0].shape[0]\n",
    "   \n",
    "    error_rate = N_error/np.shape(datas)[0]\n",
    "    return error_rate\n",
    "\n",
    "    \n",
    "def load_dataset_iris(file_data,N_train):\n",
    "    # 数据读取\n",
    "    datas = np.loadtxt(file_data,dtype = np.float16, delimiter = ',',usecols=(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14),encoding='utf-8')\n",
    "    labs = np.loadtxt(file_data,dtype =np.float16, delimiter = ',',usecols=(15,16))\n",
    "    N,D = np.shape(datas)\n",
    "    N_test = N-N_train\n",
    "    unqiue_labs = np.unique(labs).tolist()\n",
    "    \n",
    "    dic_str2index={}\n",
    "    dic_index2str={}\n",
    "    for i in range(len(unqiue_labs)):\n",
    "        lab_str = unqiue_labs[i]\n",
    "        dic_str2index[lab_str] =i\n",
    "        dic_index2str[i]=lab_str\n",
    "    \n",
    "    labs_onehot = np.zeros([N,len(unqiue_labs)])\n",
    "    for i in range(N):\n",
    "        labs_onehot[i,dic_str2index[labs[i]]]=1\n",
    "    \n",
    "    perm = np.random.permutation(N)\n",
    "    index_train = perm[:N_train]\n",
    "    index_test = perm[N_train:]\n",
    "\n",
    "    data_train = datas[index_train,:]\n",
    "    lab_train_onehot = labs_onehot[index_train,:]\n",
    "\n",
    "    data_test = datas[index_test,:]\n",
    "    lab_test_onehot = labs_onehot[index_test]\n",
    "\n",
    "    return data_train,lab_train_onehot,data_test,lab_test_onehot,dic_index2str\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    file_data = r'E:\\pxdata\\use\\data_t1.txt'\n",
    "\n",
    "    data_train,lab_train_onehot,data_test,lab_test_onehot,dic_index2str =load_dataset_iris(file_data,500)\n",
    "    \n",
    "    N,dim_in = np.shape(data_train)\n",
    "    # 定义网络结构\n",
    "    list_num_hidden=[15,15,30]\n",
    "    list_act_funs =[relu,relu,no_active]\n",
    "    list_de_act_funs=[de_relu,de_relu,de_no_active]\n",
    "    \n",
    "    # 定义损失函数\n",
    "    loss_fun = loss_CE\n",
    "    de_loss_fun=de_loss_CE\n",
    "    \n",
    "    # loss_fun = loss_L2\n",
    "    # de_loss_fun=de_loss_L2\n",
    "    \n",
    "    layers = bulid_net(dim_in,list_num_hidden,\n",
    "          list_act_funs,list_de_act_funs)\n",
    "    \n",
    "   \n",
    "    # 进行训练\n",
    "    n_epoch = 200\n",
    "    batchsize =4    \n",
    "    N_batch = N//batchsize\n",
    "    for i in range(n_epoch):\n",
    "        # 数据打乱\n",
    "        rand_index  = np.random.permutation(N).tolist()\n",
    "        # 每个batch 更新一下weight\n",
    "        loss_sum =0\n",
    "        for j in range(N_batch):\n",
    "            index = rand_index[j*batchsize:(j+1)*batchsize]\n",
    "            batch_datas = data_train[index]\n",
    "            batch_labs = lab_train_onehot[index]\n",
    "            layers,loss = updata_wb(batch_datas,batch_labs,layers,loss_fun,de_loss_fun,alpha=0.01)\n",
    "            loss_sum = loss_sum+loss\n",
    "            \n",
    "        error = test_accuracy(data_train,lab_train_onehot,layers)\n",
    "        print(\"epoch %d  error  %.2f%%  loss_all %.2f\"%(i,error*100,loss_sum))\n",
    "    \n",
    "    #进行测试\n",
    "    error = test_accuracy(data_test,lab_test_onehot,layers)\n",
    "    print(error*100)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe401327",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0xff in position 0: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-986496a8d194>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# 加载训练数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_lab_onehot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"E:\\\\pxdata\\\\use\\\\5_5no1.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"E:\\\\pxdata\\\\use\\\\label1.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-986496a8d194>\u001b[0m in \u001b[0;36mload_mnist\u001b[1;34m(file_data, file_lab)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_lab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# 加载训练数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mlab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_lab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[1;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, like)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst_vals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m                 \u001b[0mfirst_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m                 \u001b[0mfirst_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_line\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0xff in position 0: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "from NN_BP import *\n",
    "\n",
    "def load_mnist(file_data,file_lab):\n",
    "    # 加载训练数据\n",
    "    data = np.loadtxt(file_data)\n",
    "    lab = np.loadtxt(file_lab)\n",
    "    N,D = np.shape(data)\n",
    "    \n",
    "    # 构造 one-hot 标签\n",
    "    lab_onehot = np.zeros([N,10])\n",
    "    for i in range(N):\n",
    "        id = int(lab[i,0])\n",
    "        lab_onehot[i,id]=1\n",
    "    data = (data.astype(np.float)/255.0)\n",
    "    return data,lab_onehot\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    # 加载训练数据\n",
    "    train_data,train_lab_onehot=load_mnist(\"E:\\\\pxdata\\\\use\\\\5_5no1.txt\",\"E:\\\\pxdata\\\\use\\\\label1.txt\")\n",
    "    N,D = np.shape(train_data) \n",
    "    \n",
    "    # 搭建网络\n",
    "    # 定义网络结构\n",
    "    list_num_hidden=[30,5,10]\n",
    "    \n",
    "    # list_act_funs =[sigmod,sigmod,no_active]\n",
    "    # list_de_act_funs=[de_sigmoid,de_sigmoid,de_no_active]\n",
    "    \n",
    "    # # 定义损失函数\n",
    "    # loss_fun = loss_L2\n",
    "    # de_loss_fun=de_loss_L2\n",
    "    \n",
    "    list_act_funs =[relu,relu,no_active]\n",
    "    list_de_act_funs=[de_relu,de_relu,de_no_active]\n",
    "    # 定义损失函数\n",
    "    loss_fun = loss_CE\n",
    "    de_loss_fun=de_loss_CE\n",
    "    \n",
    "    layers = bulid_net(D,list_num_hidden,\n",
    "          list_act_funs,list_de_act_funs)\n",
    "          \n",
    "    # 进行训练\n",
    "    n_epoch = 50\n",
    "    batchsize =20    \n",
    "    N_batch = N//batchsize\n",
    "    for i in range(n_epoch):\n",
    "        # 数据打乱\n",
    "        rand_index  = np.random.permutation(N).tolist()\n",
    "        # 每个batch 更新一下weight\n",
    "        loss_sum =0\n",
    "        for j in range(N_batch):\n",
    "            index = rand_index[j*batchsize:(j+1)*batchsize]\n",
    "            batch_datas = train_data[index]\n",
    "            batch_labs = train_lab_onehot[index]\n",
    "            layers,loss = updata_wb(batch_datas,batch_labs,layers,loss_fun,de_loss_fun,alpha=0.001)\n",
    "            # print(\"epoch %d  batch %d  loss %.2f\"%(i,j,loss/batchsize))\n",
    "            loss_sum = loss_sum+loss\n",
    "            \n",
    "        error = test_accuracy(train_data,train_lab_onehot,layers)\n",
    "        print(\"epoch %d  error  %.2f%%  loss_all %.2f\"%(i,error*100,loss_sum/(N_batch*batchsize)))\n",
    "        \n",
    "    np.save(\"model.npy\",layers)    \n",
    "    \n",
    "    # 加载测试数据\n",
    "    #test_data,test_lab_onehot=load_mnist(\"test_data.npy\",\"test_lab.npy\")\n",
    "    #layers = np.load(\"model.npy\",allow_pickle=True)\n",
    "   \n",
    "    #error = test_accuracy(test_data,test_lab_onehot,layers)\n",
    "   # print(\"Accuarcy on Test Data %.2f %%\"%((1-error)*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db4e6013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-014c25cafec8>:150: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  datas = np.loadtxt(file_data,dtype = np.float, delimiter = ',',usecols=(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14))\n",
      "<ipython-input-30-014c25cafec8>:41: RuntimeWarning: overflow encountered in exp\n",
      "  p = np.exp(o)/np.sum(np.exp(o),axis=1,keepdims=True)\n",
      "<ipython-input-30-014c25cafec8>:41: RuntimeWarning: invalid value encountered in true_divide\n",
      "  p = np.exp(o)/np.sum(np.exp(o),axis=1,keepdims=True)\n",
      "<ipython-input-30-014c25cafec8>:42: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_ce = np.sum(-lab*np.log(p))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,2) (4,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-014c25cafec8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    215\u001b[0m             \u001b[0mbatch_datas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[0mbatch_labs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlab_train_onehot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdata_wb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_datas\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_labs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mde_loss_fun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m             \u001b[0mloss_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_sum\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-014c25cafec8>\u001b[0m in \u001b[0;36mupdata_wb\u001b[1;34m(datas, labs, layers, loss_fun, de_loss_fun, alpha)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_acfun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfead_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;31m# 计算 loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m     \u001b[1;31m#从后向前计算\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[0mdeltas0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mde_loss_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-014c25cafec8>\u001b[0m in \u001b[0;36mloss_CE\u001b[1;34m(o, lab)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss_CE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mloss_ce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlab\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss_ce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,2) (4,3) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmod(z):\n",
    "    h = 1./(1+np.exp(-z))\n",
    "    return h\n",
    "    \n",
    "def de_sigmoid(z,h):\n",
    "    return h*(1-h)\n",
    "    \n",
    "    \n",
    "def relu(z):\n",
    "    h = np.maximum(z, 0)\n",
    "    return h\n",
    "    \n",
    "def de_relu(z,h):\n",
    "    z[z <= 0] = 0\n",
    "    z[z > 0] = 1.0\n",
    "    return z\n",
    "    \n",
    "    \n",
    "    \n",
    "def no_active(z):\n",
    "    h = z\n",
    "    return h\n",
    "\n",
    "def de_no_active(z,h):\n",
    "    return np.ones(h.shape)\n",
    "    \n",
    "# o Nxc\n",
    "# lab Nxc    \n",
    "def loss_L2(o,lab):\n",
    "    diff = lab-o\n",
    "    sqrDiff = diff ** 2\n",
    "    return 0.5*np.sum(sqrDiff)\n",
    "    \n",
    "def de_loss_L2(o,lab):\n",
    "    return o-lab\n",
    "\n",
    "\n",
    "def loss_CE(o,lab):    \n",
    "    p = np.exp(o)/np.sum(np.exp(o),axis=1,keepdims=True)\n",
    "    loss_ce = np.sum(-lab*np.log(p))\n",
    "    return loss_ce\n",
    "\n",
    "def de_loss_CE(o,lab):\n",
    "    p = np.exp(o)/np.sum(np.exp(o),axis=1,keepdims=True)\n",
    "    return p-lab\n",
    "\n",
    "# dim_in:输入特征的维度\n",
    "# list_num_hidden： 每层输出节点的数目\n",
    "# list_act_funs： 每层的激活函数\n",
    "# list_de_act_funs: 反向传播时的函数\n",
    "\n",
    "def bulid_net(dim_in,list_num_hidden,\n",
    "              list_act_funs,list_de_act_funs):\n",
    "    layers=[]          \n",
    "    \n",
    "    # 逐层的进行网络构建\n",
    "    for i in range(len(list_num_hidden)):\n",
    "        layer = {}\n",
    "        \n",
    "        # 定义每一层的权重\n",
    "        if i ==0:\n",
    "            # layer[\"w\"]= 0.2*np.random.randn(dim_in,list_num_hidden[i])-0.1 # 用sigmoid激活函数\n",
    "            layer[\"w\"]= 0.01*np.random.randn(dim_in,list_num_hidden[i])  # 用relu 激活函数\n",
    "        else:\n",
    "            # layer[\"w\"]= 0.2*np.random.randn(list_num_hidden[i-1],list_num_hidden[i])-0.1 # 用sigmoid激活函数\n",
    "            layer[\"w\"]= 0.01*np.random.randn(list_num_hidden[i-1],list_num_hidden[i]) # 用relu 激活函数\n",
    "        \n",
    "        # 定义每一层的偏置\n",
    "        layer[\"b\"] = 0.1*np.ones([1,list_num_hidden[i]])\n",
    "        layer[\"act_fun\"]= list_act_funs[i]\n",
    "        layer[\"de_act_fun\"]= list_de_act_funs[i]\n",
    "        layers.append(layer)\n",
    "        \n",
    "    return layers\n",
    "    \n",
    "    \n",
    "# 返回每一层的输入\n",
    "# 与最后一层的输出    \n",
    "def fead_forward(datas,layers):\n",
    "    input_layers = []\n",
    "    input_acfun = []\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if i ==0:\n",
    "            inputs = datas\n",
    "            z = np.dot(inputs,layer[\"w\"]) + layer[\"b\"]\n",
    "            h = layer['act_fun'](z)\n",
    "            input_layers.append(inputs)\n",
    "            input_acfun.append(z)\n",
    "        else:\n",
    "            inputs = h\n",
    "            z = np.dot(inputs,layer[\"w\"])+ layer[\"b\"]\n",
    "            h = layer['act_fun'](z)\n",
    "            input_layers.append(inputs)\n",
    "            input_acfun.append(z)\n",
    "    return input_layers,input_acfun,h\n",
    "\n",
    "\n",
    "# 进行参数更新更新    \n",
    "def updata_wb(datas,labs,layers, loss_fun,de_loss_fun,alpha=0.01):\n",
    "    N,D = np.shape(datas)\n",
    "    # 进行前馈操作\n",
    "    inputs,input_acfun,output = fead_forward(datas,layers)\n",
    "    # 计算 loss\n",
    "    loss = loss_fun(output,labs)\n",
    "    #从后向前计算\n",
    "    deltas0 = de_loss_fun(output,labs)\n",
    "    # 从后向前计算误差\n",
    "    deltas =[]\n",
    "    for i in range(len(layers)):\n",
    "        index = -i-1\n",
    "        if i ==0:\n",
    "            h = output\n",
    "            z = input_acfun[index]\n",
    "            delta = deltas0*layers[index][\"de_act_fun\"](z,h)\n",
    "        else:\n",
    "            h = inputs[index+1]\n",
    "            z = input_acfun[index]\n",
    "            # print(layers[index][\"de_act_fun\"](z,h)[1])\n",
    "            delta = np.dot(delta,layers[index+1][\"w\"].T)*layers[index][\"de_act_fun\"](z,h)\n",
    "        \n",
    "        deltas.insert(0,delta)\n",
    "    \n",
    "    # 利用误差 对每一层的权重进行修成\n",
    "    for i in range(len(layers)):\n",
    "        # 计算 dw 与 db\n",
    "        dw = np.dot(inputs[i].T,deltas[i])\n",
    "        db = np.sum(deltas[i],axis=0,keepdims=True)\n",
    "        # 梯度下降\n",
    "        layers[i][\"w\"] = layers[i][\"w\"] - alpha*dw\n",
    "        layers[i][\"b\"] = layers[i][\"b\"] - alpha*db\n",
    "        \n",
    "    return layers,loss\n",
    "    \n",
    "    \n",
    "def test_accuracy(datas,labs_true,layers):\n",
    "    _,_,output = fead_forward(datas,layers)\n",
    "    lab_det = np.argmax(output,axis=1)\n",
    "    labs_true = np.argmax(labs_true,axis=1) \n",
    "    N_error = np.where(np.abs(labs_true-lab_det)>0)[0].shape[0]\n",
    "   \n",
    "    error_rate = N_error/np.shape(datas)[0]\n",
    "    return error_rate\n",
    "\n",
    "    \n",
    "def load_dataset_iris(file_data,N_train):\n",
    "    # 数据读取\n",
    "    datas = np.loadtxt(file_data,dtype = np.float, delimiter = ',',usecols=(0,1,2,3,4,5,6,7,8,9,10,11,12,13,14))\n",
    "    labs = np.loadtxt(file_data,dtype = str, delimiter = ',',usecols=(15))\n",
    "    N,D = np.shape(datas)\n",
    "    N_test = N-N_train\n",
    "    unqiue_labs = np.unique(labs).tolist()\n",
    "    \n",
    "    dic_str2index={}\n",
    "    dic_index2str={}\n",
    "    for i in range(len(unqiue_labs)):\n",
    "        lab_str = unqiue_labs[i]\n",
    "        dic_str2index[lab_str] =i\n",
    "        dic_index2str[i]=lab_str\n",
    "    \n",
    "    labs_onehot = np.zeros([N,len(unqiue_labs)])\n",
    "    for i in range(N):\n",
    "        labs_onehot[i,dic_str2index[labs[i]]]=1\n",
    "    \n",
    "    perm = np.random.permutation(N)\n",
    "    index_train = perm[:N_train]\n",
    "    index_test = perm[N_train:]\n",
    "\n",
    "    data_train = datas[index_train,:]\n",
    "    lab_train_onehot = labs_onehot[index_train,:]\n",
    "\n",
    "    data_test = datas[index_test,:]\n",
    "    lab_test_onehot = labs_onehot[index_test]\n",
    "\n",
    "    return data_train,lab_train_onehot,data_test,lab_test_onehot,dic_index2str\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    file_data = 'data_t1.data'\n",
    "\n",
    "    data_train,lab_train_onehot,data_test,lab_test_onehot,dic_index2str =load_dataset_iris(file_data,10)\n",
    "    \n",
    "    N,dim_in = np.shape(data_train)\n",
    "    # 定义网络结构\n",
    "    list_num_hidden=[15,15,3]\n",
    "    list_act_funs =[relu,relu,no_active]\n",
    "    list_de_act_funs=[de_relu,de_relu,de_no_active]\n",
    "    \n",
    "    # 定义损失函数\n",
    "    loss_fun = loss_CE\n",
    "    de_loss_fun=de_loss_CE\n",
    "    \n",
    "    # loss_fun = loss_L2\n",
    "    # de_loss_fun=de_loss_L2\n",
    "    \n",
    "    layers = bulid_net(dim_in,list_num_hidden,\n",
    "          list_act_funs,list_de_act_funs)\n",
    "    \n",
    "   \n",
    "    # 进行训练\n",
    "    n_epoch = 50\n",
    "    batchsize =4    \n",
    "    N_batch = N//batchsize\n",
    "    for i in range(n_epoch):\n",
    "        # 数据打乱\n",
    "        rand_index  = np.random.permutation(N).tolist()\n",
    "        # 每个batch 更新一下weight\n",
    "        loss_sum =0\n",
    "        for j in range(N_batch):\n",
    "            index = rand_index[j*batchsize:(j+1)*batchsize]\n",
    "            batch_datas = data_train[index]\n",
    "            batch_labs = lab_train_onehot[index]\n",
    "            layers,loss = updata_wb(batch_datas,batch_labs,layers,loss_fun,de_loss_fun,alpha=0.01)\n",
    "            loss_sum = loss_sum+loss\n",
    "            \n",
    "        error = test_accuracy(data_train,lab_train_onehot,layers)\n",
    "        print(\"epoch %d  error  %.2f%%  loss_all %.2f\"%(i,error*100,loss_sum))\n",
    "    \n",
    "    #进行测试\n",
    "    error = test_accuracy(data_test,lab_test_onehot,layers)\n",
    "    print(error*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1efae993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b8c82ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define the quadratic and cross-entropy cost functions\n",
    "class CrossEntropyCost(object):\n",
    " \n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    " \n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        return (a-y)\n",
    " \n",
    "#### Main Network class\n",
    "class Network(object):\n",
    " \n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    " \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    " \n",
    "    def default_weight_initializer(self):\n",
    " \n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "    def large_weight_initializer(self):\n",
    " \n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]): # 前n-1层\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    " \n",
    "        b = self.biases[-1]   # 最后一层\n",
    "        w = self.weights[-1]\n",
    "        a = np.dot(w, a)+b\n",
    "        return a\n",
    " \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_accuracy=False):  # 用随机梯度下降算法进行训练\n",
    " \n",
    "        n = len(training_data)\n",
    " \n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
    "            print (\"Epoch %s training complete\" % j)\n",
    "            \n",
    "            if monitor_evaluation_accuracy:\n",
    "                print (\"Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), j))\n",
    "         \n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    " \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):    # 正向传播 前n-1层\n",
    " \n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "# 最后一层，不用非线性\n",
    "        b = self.biases[-1]\n",
    "        w = self.weights[-1]\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        activation = z\n",
    "        activations.append(activation)\n",
    "        # backward pass 反向传播\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)   # 误差 Tj - Oj \n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())  #  (Tj - Oj) * O(j-1)\n",
    " \n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]    # w*a + b\n",
    "            sp = sigmoid_prime(z)  # z * (1-z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp  # z*(1-z)*(Err*w) 隐藏层误差\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())  # Errj * Oi\n",
    "        return (nabla_b, nabla_w)\n",
    " \n",
    "    def accuracy(self, data):\n",
    " \n",
    "        results = [(self.feedforward(x), y) for (x, y) in data]  \n",
    "        alist=[np.sqrt((x[0][0]-y[0])**2+(x[1][0]-y[1])**2) for (x,y) in results]\n",
    " \n",
    "        return np.mean(alist)\n",
    " \n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    " \n",
    "##Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    " \n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\" \n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "  \n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e7df667",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'my_datas_loader_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-411a7c05e5eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#coding: utf8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmy_datas_loader_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetwork_0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'my_datas_loader_1'"
     ]
    }
   ],
   "source": [
    "#coding: utf8\n",
    "import my_datas_loader_1\n",
    "import network_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6552c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn.datasets import load_iris\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "451ad7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_train(feature, label, n_hidden, maxCycle, alpha, n_output):\n",
    "    \"\"\"计算隐含层的输入\n",
    "    input:  feature(mat): 特征\n",
    "            label(mat):  标签\n",
    "            n_hidden(int): 隐含层的节点数\n",
    "            maxCycle(int):  最大的迭代次数\n",
    "            alpha(float):  学习率\n",
    "            n_output(int):  输出层的节点数\n",
    "    output:  w0(mat):  输入层到隐含层之间的的权重\n",
    "             b0(mat):  输入层到隐含层之间的偏置\n",
    "             w1(mat):  隐含层到输出层之间的权重\n",
    "             b1(mat):  隐含层到输出层之间的偏置\n",
    "    \"\"\"\n",
    "    m, n = np.shape(feature)\n",
    "    #1. 随机初始化参数（权重，偏置， 网络层结构， 激活函数）\n",
    "    w0 = np.mat(np.random.rand(n, n_hidden))\n",
    "    w0 = w0 * (8.0 * sqrt(6) / sqrt(n + n_hidden)) -\\\n",
    "         np.mat(np.ones((n, n_hidden))) * (4.0 * sqrt(6) / sqrt(n + n_hidden))\n",
    "    b0 = np.mat(np.random.rand(1, n_hidden))\n",
    "    b0 = b0 * (8.0 * sqrt(6) / sqrt(n + n_hidden)) -\\\n",
    "         np.mat(np.ones((1, n_hidden))) * (4.0 * sqrt(6) / sqrt(n + n_hidden))\n",
    "    w1 = np.mat(np.random.rand(n_hidden, n_output))\n",
    "    w1 = w1 * (8.0 * sqrt(6) / sqrt(n_hidden + n_output)) -\\\n",
    "         np.mat(np.ones((n_hidden, n_output))) * (4.0 * sqrt(6) / sqrt(n_hidden + n_output))\n",
    "    b1 = np.mat(np.random.rand(1, n_output))\n",
    "    b1 = b1 * (8.0 * sqrt(6) / sqrt(n_hidden + n_output)) -\\\n",
    "         np.mat(np.ones((1, n_output))) * (4.0 * sqrt(6) / sqrt(n_hidden + n_output))\n",
    "    #2. 训练\n",
    "    i = 0\n",
    "    while i <= maxCycle:\n",
    "        #信号正向传播\n",
    "        #计算隐含层的输入\n",
    "        hidden_input = hidden_in(feature, w0, b0)\n",
    "        #计算隐含层的输出\n",
    "        hidden_output = hidden_out(hidden_input)\n",
    "        #计算输出层的输入\n",
    "        output_in = predict_in(hidden_output, w1, b1)\n",
    "        #计算输出层的输出\n",
    "        output_out = predict_out(output_in)\n",
    "\n",
    "        #误差的反向传播\n",
    "        #隐含层到输出层之间的残差\n",
    "        delta_output = -np.multiply((label - output_out), partial_sig(output_in))\n",
    "        #输入层到隐含层之间的残差\n",
    "        delta_hidden = np.multiply((delta_output * w1.T), partial_sig(hidden_input))\n",
    "\n",
    "        #修正权重和偏置\n",
    "        w1 = w1 - alpha * (hidden_output.T * delta_output)\n",
    "        b1 = b1 - alpha * np.sum(delta_output, axis=0) * (1.0 / m)\n",
    "        w0 = w0 - alpha * (feature.T * delta_hidden)\n",
    "        b0 = b0 - alpha * np.sum(delta_hidden, axis=0) * (1.0 / m)\n",
    "        if i % 100 == 0:\n",
    "            print(\"\\t------iter: \", i, \", cost: \", (1.0/2) * get_cost(get_predict(feature, w0, w1, b0, b1) - label))\n",
    "        i += 1\n",
    "    return w0, w1, b0, b1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62596a5",
   "metadata": {},
   "source": [
    "## 计算隐含层的输入的hidden_in函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b79035ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_in(feature, w0, b0):\n",
    "    \"\"\"隐含层的输入\n",
    "    input:  feature(mat): 特征\n",
    "            w0(mat): 输入层到隐含层之间的权重\n",
    "            b0(mat): 输入层到隐含层之间的偏置\n",
    "    output:  hidden_in(mat): 隐含层的输入\n",
    "    \"\"\"\n",
    "    m = np.shape(feature)[0]\n",
    "    hidden_in = feature * w0\n",
    "    for i in range(m):\n",
    "        hidden_in[i, ] += b0\n",
    "    return hidden_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a8c10",
   "metadata": {},
   "source": [
    "# 计算隐含层的输出的hidden_out函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3713163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_out(hidden_in):\n",
    "    \"\"\"隐含层的输出\n",
    "    input:  hidden_in(mat): 隐含层的输入\n",
    "    output: hidden_output(mat): 隐含层的输出\n",
    "    \"\"\"\n",
    "    hidden_output = sig(hidden_in)\n",
    "    return hidden_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f97a8c4",
   "metadata": {},
   "source": [
    "# 计算输出层的输入的predict_in函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f325b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_in(hidden_out, w1, b1):\n",
    "    \"\"\"输出层的输入\n",
    "    input:  hidden_out(mat): 隐含层的输出\n",
    "            w1(mat): 隐含层到输出层之间的权重\n",
    "            b1(mat): 隐含层到输出层之间的偏置\n",
    "    output:  predict_in(mat): 输出层的输入\n",
    "    \"\"\"\n",
    "    m = np.shape(hidden_out)[0]\n",
    "    predict_in = hidden_out * w1\n",
    "    for i in range(m):\n",
    "        predict_in[i, ] += b1\n",
    "    return predict_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12c1dd5",
   "metadata": {},
   "source": [
    "# 计算输出层的输出的predict_out函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4cd6c5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_out(predict_in):\n",
    "    \"\"\"输出层的输出\n",
    "    input:  predict_in(mat): 输出层的输入\n",
    "    output:  result(mat): 输出层的输出\n",
    "    \"\"\"\n",
    "    result = sig(predict_in)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa569ab7",
   "metadata": {},
   "source": [
    "# Sigmoid函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c922a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(x):\n",
    "    \"\"\"Sigmoid激活函数\n",
    "    input:  x(mat/float): 自变量（矩阵或者任意实数）\n",
    "    output: Sigmoid值（mat/float）: Sigmoid函数的值\n",
    "    \"\"\"\n",
    "    return 1.0 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f1320",
   "metadata": {},
   "source": [
    "# partial_sig函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eeede2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_sig(x):\n",
    "    m, n = np.shape(x)\n",
    "    out = np.mat(np.zeros((m, n)))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            out[i, j] = sig(x[i, j]) * (1 - sig(x[i, j]))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b3a96f",
   "metadata": {},
   "source": [
    "# 计算损失函数值的get_cost函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13ad8fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost(cost):\n",
    "    m, n = np.shape(cost)\n",
    "    cost_sum = 0.0\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            cost_sum += cost[i, j] * cost[i, j]\n",
    "    return cost_sum / m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe08de1",
   "metadata": {},
   "source": [
    "# 计算错误率的err_rate函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9595e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def err_rate(label, pre):\n",
    "    m = np.shape(label)[0]\n",
    "    err = 0.0\n",
    "    for i in range(m):\n",
    "        if label[i, 0] != pre[i, 0]:\n",
    "            err += 1\n",
    "    rate = err / m\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20453a5f",
   "metadata": {},
   "source": [
    "# 导入数据的load_data函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c24e03f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    dataset_iris = load_iris()\n",
    "    data_iris = dataset_iris['data']\n",
    "    label_tmp = dataset_iris['target']\n",
    "    m = len(label_tmp)\n",
    "    n_class = len(set(label_tmp))\n",
    "    print(label_tmp) \n",
    "    label_iris = np.mat(np.zeros((m, n_class)))\n",
    "    for i in range(m):\n",
    "        label_iris[i, label_tmp[i]] = 1\n",
    "    print(label_iris)\n",
    "    return data_iris, label_iris, n_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94f8b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存bp模型的save_model函数\n",
    "def save_model(w0, w1, b0, b1):\n",
    "    def write_file(filename, source):\n",
    "        f = open(filename, \"w\")\n",
    "        m, n = np.shape(source)\n",
    "        for i in range(m):\n",
    "            tmp = []\n",
    "            for j in range(n):\n",
    "                tmp.append(str(source[i, j]))\n",
    "            f.write(\"\\t\".join(tmp) + \"\\n\")\n",
    "        f.close()\n",
    "    write_file(\"weight_w0\", w0)\n",
    "    write_file(\"weight_w1\", w1)\n",
    "    write_file(\"weight_b0\", b0)\n",
    "    write_file(\"weight_b1\", b1)\n",
    "\n",
    "#对测试样本进行预测的get_predict函数\n",
    "def get_predict(feature, w0, w1, b0, b1):\n",
    "    return predict_out(predict_in(hidden_out(hidden_in(feature, w0, b0)), w1, b1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e1bb86",
   "metadata": {},
   "source": [
    "# 训练BP模型的主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e077ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------1. load Data-------------\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "----------2. training--------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-fdeb5fe68a88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#2.训练模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"----------2. training--------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbp_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m#3.保存模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"----------3. save model-------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "#训练BP模型的主函数\n",
    "if __name__ == \"__main__\":\n",
    "    #1.导入数据\n",
    "    print(\"----------1. load Data-------------\")\n",
    "    feature, label, n_class = load_data()\n",
    "    #2.训练模型\n",
    "    print(\"----------2. training--------------\")\n",
    "    w0, w1, b0, b1 = bp_train(feature, label, 30, 1000, 0.01, n_class)\n",
    "    #3.保存模型\n",
    "    print(\"----------3. save model-------------\")\n",
    "    save_model(w0, w1, b0, b1)\n",
    "    #4.得到最终的预测结果\n",
    "    print(\"----------4. get prediction----------\")\n",
    "    result = get_predict(feature, w0, w1, b0, b1)\n",
    "    print(\"训练准确性为： \", (1 - err_rate(np.argmax(label, axis=1), np.argmax(result, axis=1))))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fea145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------1. load Data-------------\n",
      "feature:  [[0.05261478 0.87755822 0.49340511 0.78464419 0.67261779 0.05261549\n",
      "  0.71827411 0.17403481 0.44169611 0.70079513 0.05261558 0.62991041\n",
      "  0.41286111 0.44933921 0.70313799]\n",
      " [0.19120343 0.         0.01000046 0.01498127 0.04939048 0.19064568\n",
      "  0.6998731  0.1410462  0.41813899 0.55569561 0.19064611 0.65816678\n",
      "  0.32134958 0.43281938 0.59809266]\n",
      " [0.31898005 0.99894143 0.69235837 0.96629213 0.70557917 0.31898101\n",
      "  0.93432741 0.26498258 0.58892815 0.74145385 0.31898094 0.98414886\n",
      "  0.6256806  0.6123348  0.82388308]\n",
      " [0.84685413 0.93013409 0.6430282  0.8071161  0.70902865 0.84685423\n",
      "  0.88261421 0.29364703 0.57714959 0.88675398 0.84685416 0.81977946\n",
      "  0.64670587 0.54185022 0.80960964]\n",
      " [1.         0.85956246 0.49658798 0.80337079 0.79000732 1.\n",
      "  0.81376904 0.21581378 0.49234393 0.74808995 1.         0.75740868\n",
      "  0.48673251 0.51101322 0.83074777]\n",
      " [0.04074337 0.85673959 0.54101369 0.72846442 0.72720353 0.04074419\n",
      "  0.77252538 0.24502415 0.47585395 0.77119722 0.04074416 0.70640937\n",
      "  0.63552273 0.45814978 0.79758494]\n",
      " [0.11034961 0.88002823 0.49952255 0.70973783 0.73395626 0.11035023\n",
      "  0.82074873 0.23348945 0.4664311  0.68250884 0.11035034 0.75982081\n",
      "  0.59476345 0.4746696  0.79276539]\n",
      " [0.43383017 0.96189132 0.47680776 0.71348315 0.63548579 0.4338308\n",
      "  0.82963198 0.25283601 0.48174323 0.73239545 0.43383053 0.80737422\n",
      "  0.6665329  0.50770925 0.82126051]\n",
      " [0.56952553 0.78687368 0.2897719  0.51498127 0.53451134 0.56952632\n",
      "  0.6643401  0.16143258 0.38162544 0.65620152 0.56952617 0.58097864\n",
      "  0.34366887 0.34140969 0.53628633]\n",
      " [0.62688442 0.83662668 0.28861831 0.56928839 0.53202939 0.62688468\n",
      "  0.70114213 0.15061144 0.38869258 0.60325647 0.62688478 0.66919366\n",
      "  0.39351307 0.40969163 0.74397068]\n",
      " [0.05082923 0.32498236 0.03512964 0.06179775 0.13344805 0.05083131\n",
      "  0.16497462 0.01712967 0.04475854 0.18302699 0.05083173 0.09269469\n",
      "  0.04289417 0.05837004 0.17160794]\n",
      " [0.11350725 1.         0.48249581 0.65543071 0.61343675 0.11350819\n",
      "  0.82677665 0.23126162 0.49469965 0.70886551 0.11350814 0.81977946\n",
      "  0.6462965  0.48348018 0.74763423]\n",
      " [0.37047696 0.93330981 0.3886605  0.65168539 0.60709267 0.37047777\n",
      "  0.70241117 0.19050384 0.37102473 0.51965835 0.37047832 0.71399035\n",
      "  0.44895287 0.40638767 0.63094032]\n",
      " [0.44381117 0.93966126 0.3836695  0.5917603  0.56161841 0.44381173\n",
      "  0.74714467 0.21773203 0.42285041 0.62221967 0.44381176 0.70089593\n",
      "  0.54735548 0.40088106 0.6340897 ]\n",
      " [0.54372371 0.92801694 0.46877807 0.73970037 0.70074649 0.54372497\n",
      "  0.77760152 0.25509463 0.4958775  0.73519214 0.54372468 0.89110958\n",
      "  0.60928829 0.52753304 0.88907574]\n",
      " [0.02853661 0.12526464 0.01271343 0.03370787 0.06051816 0.02853674\n",
      "  0.         0.         0.         0.         0.02853575 0.05375603\n",
      "  0.00274839 0.03854626 0.        ]\n",
      " [0.46807527 0.65737474 0.16207644 0.41385768 0.39685761 0.46807609\n",
      "  0.50856599 0.06597196 0.26501767 0.35978691 0.46807528 0.53652653\n",
      "  0.24048036 0.30726872 0.39733625]\n",
      " [0.59545177 0.86485533 0.38925453 0.69101124 0.62983681 0.59545199\n",
      "  0.76332487 0.2024536  0.48998822 0.69997876 0.59545165 0.83184011\n",
      "  0.67586796 0.51651982 0.89639247]\n",
      " [0.66113286 0.67290049 0.32376865 0.53558052 0.58670227 0.66113294\n",
      "  0.69733503 0.17006609 0.37691402 0.74757288 0.66113234 0.70261888\n",
      "  0.57333558 0.43722467 0.91025412]\n",
      " [0.72848773 0.72441778 0.32475247 0.56367041 0.5589081  0.72848816\n",
      "  0.72112944 0.17356789 0.37573616 0.53446138 0.72848758 0.74638181\n",
      "  0.55990596 0.46255507 0.76298545]\n",
      " [0.09509908 0.70007057 0.33621822 0.59925094 0.62280163 0.09509898\n",
      "  0.74746193 0.18631474 0.39104829 0.74402848 0.09509961 0.71123363\n",
      "  0.654838   0.47356828 0.8988011 ]\n",
      " [0.16651956 0.47353564 0.12107165 0.29962547 0.28166757 0.16651923\n",
      "  0.52252538 0.08039148 0.25559482 0.47096358 0.16651986 0.53514817\n",
      "  0.25928806 0.32599119 0.57250882]\n",
      " [0.31785548 0.69442484 0.28494576 0.52434457 0.45372497 0.31785491\n",
      "  0.66465736 0.16484956 0.43109541 0.67340432 0.31785506 0.7398346\n",
      "  0.56409499 0.46696035 0.87519569]\n",
      " [0.37492987 0.92978123 0.5752425  0.89700375 0.75445984 0.37492978\n",
      "  0.84803299 0.29878666 0.70435807 0.79591042 0.37493007 1.\n",
      "  0.98662851 0.73898678 0.99561065]\n",
      " [0.4332744  0.8634439  0.55043814 0.90449438 0.7818754  0.4332744\n",
      "  0.8017132  0.29323187 0.63957597 0.93131433 0.43327455 0.93969676\n",
      "  1.         0.67400881 0.99561065]\n",
      " [0.03632651 0.48341567 0.23348664 0.45692884 0.53822624 0.03632553\n",
      "  0.57994924 0.14695076 0.3639576  0.64822555 0.03632652 0.55789111\n",
      "  0.51876136 0.49779736 0.91031509]\n",
      " [0.09871446 0.79181369 0.42312869 0.73595506 0.64956215 0.09871371\n",
      "  0.93147208 0.26968033 0.57361602 0.85973971 0.09871415 0.92694693\n",
      "  0.924593   0.65638767 0.99928685]\n",
      " [0.15237927 0.7184192  0.37117299 0.65917603 0.6132144  0.15237837\n",
      "  0.73635787 0.23941215 0.53945819 0.80451396 0.15237869 0.74982771\n",
      "  0.78178693 0.59030837 0.99776277]\n",
      " [0.25999937 0.5303458  0.17383408 0.42696629 0.41914602 0.25999877\n",
      "  0.56027919 0.12743745 0.36631331 0.62480395 0.25999874 0.60923501\n",
      "  0.42202838 0.41079295 0.77066911]\n",
      " [0.31685179 0.68983769 0.35391731 0.68726592 0.67305448 0.31685107\n",
      "  0.8194797  0.23578631 0.52532391 0.7625851  0.31685128 0.79531358\n",
      "  0.80565701 0.58039648 1.        ]\n",
      " [0.         0.82956951 0.44252979 0.82209738 0.64564092 0.\n",
      "  0.86294416 0.35836867 0.60895171 0.78809643 0.         0.89972433\n",
      "  0.95766781 0.7092511  0.99735443]\n",
      " [0.30768419 0.58786168 0.20596308 0.37265918 0.37694997 0.30768188\n",
      "  0.59739848 0.17490206 0.36984688 0.67843557 0.30768297 0.5992419\n",
      "  0.53376825 0.48568282 0.91480221]\n",
      " [0.3887912  0.49470713 0.16125658 0.37078652 0.37217338 0.38878938\n",
      "  0.51808376 0.13640346 0.36513545 0.62033376 0.38878942 0.61578222\n",
      "  0.39444002 0.43722467 0.73861397]\n",
      " [0.46822674 0.63726182 0.31041201 0.54868914 0.54871791 0.46822536\n",
      "  0.73508883 0.26124249 0.4852768  0.81475669 0.46822499 0.82701585\n",
      "  0.75811404 0.53524229 0.99629965]\n",
      " [0.5736363  0.39520113 0.13533624 0.33146067 0.3452496  0.5736349\n",
      "  0.54822335 0.13440784 0.33215548 0.60807639 0.57363462 0.56133701\n",
      "  0.35015687 0.35792952 0.72931881]\n",
      " [0.04790109 0.92731122 1.         1.         1.         0.04790143\n",
      "  0.79949239 0.37945718 0.61955241 0.99781049 0.04790137 0.70296347\n",
      "  0.66211665 0.48127753 0.79714324]\n",
      " [0.29882559 0.68807339 0.34005118 0.5411985  0.51088179 0.2988252\n",
      "  0.61484772 0.21249614 0.42756184 0.74000348 0.29882471 0.5306685\n",
      "  0.38540856 0.38215859 0.7981198 ]\n",
      " [0.35740065 0.8786168  0.51938263 0.69662921 0.71853776 0.35740057\n",
      "  0.75222081 0.30347955 0.56654888 0.87502226 0.35740001 0.63680221\n",
      "  0.51327572 0.47577093 0.87812307]\n",
      " [0.41715328 0.58645025 0.23628505 0.42883895 0.42705058 0.41715255\n",
      "  0.50634518 0.18640475 0.39458186 0.75630515 0.41715254 0.57580979\n",
      "  0.33289446 0.35022026 0.73809061]\n",
      " [0.4672389  0.92166549 0.51152915 0.7659176  0.70191635 0.46723884\n",
      "  0.7909264  0.38783676 0.66195524 0.97975056 0.46723873 0.78566506\n",
      "  0.71420684 0.54185022 0.94875871]\n",
      " [0.01734481 0.69019054 0.29643153 0.48876404 0.51141464 0.01734268\n",
      "  0.66370558 0.34354453 0.51001178 0.88389292 0.01734276 0.60785665\n",
      "  0.53432312 0.46365639 0.96409728]\n",
      " [0.15738077 0.43613267 0.18511961 0.36891386 0.41762961 0.15737904\n",
      "  0.63134518 0.22056395 0.44051826 0.86255892 0.15737791 0.43659545\n",
      "  0.34279021 0.32599119 0.74418463]\n",
      " [0.25080672 0.51270289 0.1408345  0.3071161  0.33499931 0.25080525\n",
      "  0.49206853 0.16423712 0.3745583  0.67829075 0.25080411 0.40317023\n",
      "  0.27724407 0.30066079 0.72090704]\n",
      " [0.37256532 0.70324629 0.31602485 0.59737828 0.59321862 0.37256405\n",
      "  0.87848985 0.38626042 0.61601885 1.         0.37256353 0.67091661\n",
      "  0.59602129 0.50330396 0.88525461]\n",
      " [0.44296025 0.75582216 0.29083159 0.59363296 0.57609642 0.44295993\n",
      "  0.79568528 0.3018004  0.59010601 0.80127744 0.44295947 0.70744314\n",
      "  0.47861444 0.48788546 0.81410021]\n",
      " [0.08480217 0.09774171 0.02367903 0.06179775 0.12169137 0.08479795\n",
      "  0.61199239 0.07859881 0.27797409 0.53155634 0.08480229 0.32977257\n",
      "  0.12277976 0.22136564 0.43876247]\n",
      " [0.22550129 0.34862385 0.08074943 0.23033708 0.27509012 0.22549897\n",
      "  0.73255076 0.17160945 0.38869258 0.64982718 0.22550184 0.40006892\n",
      "  0.28850053 0.37004405 0.720723  ]\n",
      " [0.29630299 0.44989414 0.08819021 0.30149813 0.30031821 0.29630166\n",
      "  0.78775381 0.14482793 0.40636042 0.61743408 0.29630323 0.51826327\n",
      "  0.22994414 0.3469163  0.5015166 ]\n",
      " [0.60076748 0.52646436 0.11851686 0.38764045 0.30517492 0.60076574\n",
      "  0.92417513 0.22696719 0.50530035 0.81050855 0.6007677  0.60096485\n",
      "  0.31721384 0.45154185 0.68379426]\n",
      " [0.64330313 0.52717008 0.13679708 0.37640449 0.36272237 0.64330169\n",
      "  0.85025381 0.22994414 0.4770318  0.77869153 0.64330356 0.58270159\n",
      "  0.33266837 0.4592511  0.75449316]\n",
      " [0.11795594 0.69689485 0.0158142  0.09925094 0.02755579 0.11795475\n",
      "  1.         0.88858199 0.8409894  0.99678815 0.11795709 0.68194349\n",
      "  0.72796331 0.92180617 0.99743955]\n",
      " [0.18452896 0.63514467 0.0498401  0.36516854 0.08576829 0.18452897\n",
      "  1.         0.86125803 0.82332155 0.9967431  0.1845298  0.84390076\n",
      "  0.66792413 0.77973568 0.99774666]\n",
      " [0.26259896 0.42519407 0.01970857 0.1928839  0.0369387  0.26259861\n",
      "  1.         0.68126892 0.80447585 0.9967431  0.26259943 0.82288077\n",
      "  0.54107548 0.71475771 0.99634796]\n",
      " [0.32912001 0.61750176 0.02651905 0.20224719 0.03427046 0.32912021\n",
      "  1.         1.         1.         0.99691152 0.32912087 0.96485183\n",
      "  0.76599472 1.         0.9956118 ]\n",
      " [0.41244392 0.32074806 0.01059227 0.07865169 0.05130653 0.41244029\n",
      "  0.78045685 0.42975031 0.65017668 0.99774077 0.41244281 0.37732598\n",
      "  0.23533353 0.51101322 0.69175513]\n",
      " [0.03249672 0.0067043  0.         0.         0.         0.03243518\n",
      "  0.4498731  0.12211965 0.30388693 0.76196719 0.03244024 0.\n",
      "  0.         0.         0.01404454]\n",
      " [0.18434216 0.49964714 0.00888626 0.08052434 0.02262796 0.18434115\n",
      "  0.90069797 0.44065077 0.65842167 0.99993993 0.18434324 0.45485872\n",
      "  0.19869804 0.35682819 0.61108242]\n",
      " [0.32299111 0.57233592 0.02832673 0.17790262 0.06276473 0.32299134\n",
      "  1.         0.6780963  0.78916372 0.9967431  0.32299178 0.61853894\n",
      "  0.29105562 0.63436123 0.65665183]\n",
      " [0.53610227 0.42907551 0.00504995 0.02996255 0.01320098 0.5361011\n",
      "  0.77696701 0.28819667 0.54652532 0.80427903 0.53610304 0.22088215\n",
      "  0.08155813 0.23678414 0.33721734]\n",
      " [0.60397898 0.45377558 0.0080486  0.07490637 0.01902523 0.60397908\n",
      "  0.89562183 0.40992061 0.65253239 0.9967431  0.60398046 0.31598897\n",
      "  0.15543292 0.34801762 0.47980105]\n",
      " [0.03676913 0.93542696 0.05558977 0.29400749 0.1064191  0.03676956\n",
      "  0.98032995 0.65631709 0.77738516 0.9967431  0.03677101 0.59338387\n",
      "  0.22271136 0.60792952 0.47847597]\n",
      " [0.15372887 0.56104446 0.03246717 0.26029963 0.0609208  0.15372893\n",
      "  1.         0.56493983 0.795053   0.9967431  0.15372927 0.71019986\n",
      "  0.2085426  0.52753304 0.45667875]\n",
      " [0.27864258 0.63761468 0.02462615 0.19475655 0.06351292 0.27864224\n",
      "  0.9930203  0.49066988 0.76796231 0.9971486  0.27864375 0.60303239\n",
      "  0.16046316 0.44933921 0.49901021]\n",
      " [0.34016206 0.42942837 0.01939709 0.20037453 0.04979112 0.3401618\n",
      "  0.95685279 0.33278034 0.75971731 0.97218652 0.34016245 0.57133012\n",
      "  0.09182579 0.3722467  0.17852899]\n",
      " [0.38850273 0.25935074 0.0133157  0.12546816 0.04419022 0.38850144\n",
      "  0.96795685 0.3693794  0.72909305 0.9967431  0.38850361 0.4007581\n",
      "  0.08701002 0.34911894 0.35083859]]\n",
      "----------2. training--------------\n",
      "\t------iter:  0 , cost:  0.20591258781458235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t------iter:  100 , cost:  0.07118729484274904\n",
      "\t------iter:  200 , cost:  0.06259744005241144\n",
      "\t------iter:  300 , cost:  0.05796540802127205\n",
      "\t------iter:  400 , cost:  0.054662634182427755\n",
      "\t------iter:  500 , cost:  0.05208577301374326\n",
      "\t------iter:  600 , cost:  0.04993826563606494\n",
      "\t------iter:  700 , cost:  0.04805936153441346\n",
      "\t------iter:  800 , cost:  0.04636884976521117\n",
      "\t------iter:  900 , cost:  0.04483019846927361\n",
      "\t------iter:  1000 , cost:  0.04342792288396499\n",
      "\t------iter:  1100 , cost:  0.042154649432645894\n",
      "\t------iter:  1200 , cost:  0.041004186779979634\n",
      "\t------iter:  1300 , cost:  0.03996852208614446\n",
      "\t------iter:  1400 , cost:  0.03903733683821998\n",
      "\t------iter:  1500 , cost:  0.038198815906957964\n",
      "\t------iter:  1600 , cost:  0.03744078724156249\n",
      "\t------iter:  1700 , cost:  0.03675164675996705\n",
      "\t------iter:  1800 , cost:  0.036120900757779593\n",
      "\t------iter:  1900 , cost:  0.03553937426255149\n",
      "\t------iter:  2000 , cost:  0.0349992022704625\n",
      "\t------iter:  2100 , cost:  0.0344937102500007\n",
      "\t------iter:  2200 , cost:  0.03401725628288907\n",
      "\t------iter:  2300 , cost:  0.0335650763280919\n",
      "\t------iter:  2400 , cost:  0.03313315307844026\n",
      "\t------iter:  2500 , cost:  0.032718116050098243\n",
      "\t------iter:  2600 , cost:  0.03231717246003358\n",
      "\t------iter:  2700 , cost:  0.03192806259272805\n",
      "\t------iter:  2800 , cost:  0.03154902889098497\n",
      "\t------iter:  2900 , cost:  0.031178785594985842\n",
      "\t------iter:  3000 , cost:  0.030816476723851908\n",
      "\t------iter:  3100 , cost:  0.030461615139130267\n",
      "\t------iter:  3200 , cost:  0.03011400306292062\n",
      "\t------iter:  3300 , cost:  0.029773641743928454\n",
      "\t------iter:  3400 , cost:  0.0294406419365527\n",
      "\t------iter:  3500 , cost:  0.02911514629682554\n",
      "\t------iter:  3600 , cost:  0.028797270849692445\n",
      "\t------iter:  3700 , cost:  0.028487067676004865\n",
      "\t------iter:  3800 , cost:  0.02818450688388969\n",
      "\t------iter:  3900 , cost:  0.027889473629345345\n",
      "\t------iter:  4000 , cost:  0.02760177532301643\n",
      "\t------iter:  4100 , cost:  0.02732115463864319\n",
      "\t------iter:  4200 , cost:  0.027047304930659703\n",
      "\t------iter:  4300 , cost:  0.02677988574753896\n",
      "\t------iter:  4400 , cost:  0.026518537060458303\n",
      "\t------iter:  4500 , cost:  0.02626289152714107\n",
      "\t------iter:  4600 , cost:  0.02601258457951228\n",
      "\t------iter:  4700 , cost:  0.025767262402963606\n",
      "\t------iter:  4800 , cost:  0.02552658801713691\n",
      "\t------iter:  4900 , cost:  0.025290245720961852\n",
      "\t------iter:  5000 , cost:  0.025057944165264082\n",
      "----------3. save model-------------\n",
      "----------4. get prediction----------\n",
      "[[21.52881613 13.71963037]\n",
      " [25.16532254  6.39950605]\n",
      " [16.89519588 11.53432551]\n",
      " [15.68594966 12.90258438]\n",
      " [16.95700002 16.15668768]\n",
      " [21.53581184 13.62384255]\n",
      " [19.38086732 13.04138924]\n",
      " [16.12445615 11.99382316]\n",
      " [16.78713844 13.42617096]\n",
      " [16.58436686 14.06702655]\n",
      " [15.94628301 12.3849479 ]\n",
      " [16.27397258 11.0191882 ]\n",
      " [14.97614024 12.85828139]\n",
      " [14.59442702 11.10062915]\n",
      " [18.06566437 16.48631996]\n",
      " [17.74972997 13.54372021]\n",
      " [15.34364326 13.71836209]\n",
      " [18.19941111 15.01838741]\n",
      " [21.39589375 15.87399391]\n",
      " [15.8108922  11.88730433]\n",
      " [25.42461888 14.6852585 ]\n",
      " [21.69821522 10.91953744]\n",
      " [22.89886596 14.23316863]\n",
      " [21.32670349 15.60197806]\n",
      " [24.24822115 17.65023751]\n",
      " [27.6371048  15.66626798]\n",
      " [25.31968847 12.29738939]\n",
      " [26.38631813 15.59009781]\n",
      " [25.66988785 15.16516849]\n",
      " [25.64067342 15.17332116]\n",
      " [25.18039896 11.98963386]\n",
      " [24.69687072 13.47955717]\n",
      " [24.92358657 15.14070626]\n",
      " [25.0904574  16.03272295]\n",
      " [24.20195079 13.45827189]\n",
      " [20.53114651 15.02313265]\n",
      " [23.75377155 15.20195024]\n",
      " [21.95627041 16.85216095]\n",
      " [24.98479605 17.55758527]\n",
      " [22.67752888 17.89963405]\n",
      " [27.2329159  16.77363588]\n",
      " [27.88452772 14.72819344]\n",
      " [25.64572404 15.04418386]\n",
      " [26.45503593 14.83995452]\n",
      " [23.50269737 15.50753159]\n",
      " [24.60268418  7.52496015]\n",
      " [25.80205627  8.32661939]\n",
      " [22.13095883  6.91033223]\n",
      " [23.09049978  6.98065847]\n",
      " [23.45937613  9.29732118]\n",
      " [20.62847397  2.46652768]\n",
      " [23.54713637  3.63076833]\n",
      " [25.24952482  4.31526718]\n",
      " [21.73129032  3.1455229 ]\n",
      " [26.50334304  7.18002743]\n",
      " [24.2685193   7.00248952]\n",
      " [23.6054915   4.97130567]\n",
      " [21.63370852  3.78095658]\n",
      " [18.29378852  4.51133703]\n",
      " [20.99253495  4.15191869]\n",
      " [17.15032865  3.6305433 ]\n",
      " [22.41978673  4.43513735]\n",
      " [21.20744416  4.82342261]\n",
      " [22.03665449  5.09680735]\n",
      " [24.363829    5.13078579]]\n"
     ]
    }
   ],
   "source": [
    "#coding: utf-8\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "#from sklearn.datasets import load_iris\n",
    "from numpy import *\n",
    "\n",
    "def standardization(data_class, handel_index):\n",
    "    \"\"\"\n",
    "    标准化\n",
    "    -数据无空值\n",
    "    -数据经过 parse 方法格式转换\n",
    "    :param data_class:\n",
    "    :param handel_index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mean_list = {}\n",
    "    std_list = {}\n",
    "    for j in handel_index:\n",
    "        col = []\n",
    "        for i in range(len(data_class.data)):\n",
    "            col.append(data_class.data[i][j])\n",
    "        mean_list[j] = np.mean(col, axis=0)\n",
    "        std_list[j] = np.std(col, axis=0)\n",
    "        col = (col - mean_list[j]) / std_list[j]\n",
    "        for i in range(len(data_class.data)):\n",
    "            data_class.data[i][j] = col[i]\n",
    "\n",
    "    data_class.standard_mean = mean_list\n",
    "    data_class.standard_std = std_list\n",
    "    return data_class\n",
    "\n",
    "#BP神经网络模型的训练\n",
    "def bp_train(feature, label, n_hidden, maxCycle, alpha, n_output):\n",
    "    \"\"\"计算隐含层的输入\n",
    "        input:  feature(mat): 特征\n",
    "                label(mat):  标签\n",
    "                n_hidden(int): 隐含层的节点数\n",
    "                maxCycle(int):  最大的迭代次数\n",
    "                alpha(float):  学习率\n",
    "                n_output(int):  输出层的节点数\n",
    "        output:  w0(mat):  输入层到隐含层之间的的权重\n",
    "                 b0(mat):  输入层到隐含层之间的偏置\n",
    "                 w1(mat):  隐含层到输出层之间的权重\n",
    "                 b1(mat):  隐含层到输出层之间的偏置\n",
    "    \"\"\"\n",
    "    m, n = np.shape(feature)\n",
    "    #1. 随机初始化参数（权重，偏置， 网络层结构， 激活函数）\n",
    "    w0 = np.mat(np.random.rand(n, n_hidden))\n",
    "    w0 = w0 * (8.0 * sqrt(6) / sqrt(n + n_hidden)) -\\\n",
    "         np.mat(np.ones((n, n_hidden))) * (4.0 * sqrt(6) / sqrt(n + n_hidden))\n",
    "    b0 = np.mat(np.random.rand(1, n_hidden))\n",
    "    b0 = b0 * (8.0 * sqrt(6) / sqrt(n + n_hidden)) -\\\n",
    "         np.mat(np.ones((1, n_hidden))) * (4.0 * sqrt(6) / sqrt(n + n_hidden))\n",
    "    w1 = np.mat(np.random.rand(n_hidden, n_output))\n",
    "    w1 = w1 * (8.0 * sqrt(6) / sqrt(n_hidden + n_output)) -\\\n",
    "         np.mat(np.ones((n_hidden, n_output))) * (4.0 * sqrt(6) / sqrt(n_hidden + n_output))\n",
    "    b1 = np.mat(np.random.rand(1, n_output))\n",
    "    b1 = b1 * (8.0 * sqrt(6) / sqrt(n_hidden + n_output)) -\\\n",
    "         np.mat(np.ones((1, n_output))) * (4.0 * sqrt(6) / sqrt(n_hidden + n_output))\n",
    "\n",
    "    #2. 训练\n",
    "    i = 0\n",
    "    while i <= maxCycle:\n",
    "        #信号正向传播\n",
    "        #计算隐含层的输入\n",
    "        hidden_input = hidden_in(feature, w0, b0)\n",
    "        #计算隐含层的输出\n",
    "        hidden_output = hidden_out(hidden_input)\n",
    "        #计算输出层的输入\n",
    "        output_in = predict_in(hidden_output, w1, b1)\n",
    "        #计算输出层的输出\n",
    "        output_out = predict_out(output_in)\n",
    "\n",
    "        #误差的反向传播\n",
    "        #隐含层到输出层之间的残差\n",
    "        delta_output = -np.multiply((label - output_out), partial_sig(output_in))\n",
    "        #输入层到隐含层之间的残差\n",
    "        delta_hidden = np.multiply((delta_output * w1.T), partial_sig(hidden_input))\n",
    "\n",
    "        #修正权重和偏置\n",
    "        w1 = w1 - alpha * (hidden_output.T * delta_output)\n",
    "        b1 = b1 - alpha * np.sum(delta_output, axis=0) * (1.0 / m)\n",
    "        w0 = w0 - alpha * (feature.T * delta_hidden) \n",
    "        b0 = b0 - alpha * np.sum(delta_hidden, axis=0) * (1.0 / m)\n",
    "        if i % 100 == 0:\n",
    "            print(\"\\t------iter: \", i, \", cost: \", (1.0/2) * get_cost(get_predict(feature, w0, w1, b0, b1) - label))\n",
    "        i += 1\n",
    "    return w0, w1, b0, b1\n",
    "\n",
    "#计算隐含层的输入的hidden_in函数\n",
    "def hidden_in(feature, w0, b0):\n",
    "    \"\"\"隐含层的输入\n",
    "    input:  feature(mat): 特征\n",
    "            w0(mat): 输入层到隐含层之间的权重\n",
    "            b0(mat): 输入层到隐含层之间的偏置\n",
    "    output:  hidden_in(mat): 隐含层的输入\n",
    "    \"\"\"\n",
    "    m = np.shape(feature)[0]\n",
    "    hidden_in = feature * w0\n",
    "    for i in range(m):\n",
    "        hidden_in[i, ] += b0\n",
    "    return hidden_in\n",
    "\n",
    "#计算隐含层的输出的hidden_out函数\n",
    "def hidden_out(hidden_in):\n",
    "    \"\"\"隐含层的输出\n",
    "    input:  hidden_in(mat): 隐含层的输入\n",
    "    output: hidden_output(mat): 隐含层的输出\n",
    "    \"\"\"\n",
    "    hidden_output = sig(hidden_in)\n",
    "    return hidden_output\n",
    "\n",
    "#计算输出层的输入的predict_in函数\n",
    "def predict_in(hidden_out, w1, b1):\n",
    "    \"\"\"输出层的输入\n",
    "    input:  hidden_out(mat): 隐含层的输出\n",
    "            w1(mat): 隐含层到输出层之间的权重\n",
    "            b1(mat): 隐含层到输出层之间的偏置\n",
    "    output:  predict_in(mat): 输出层的输入\n",
    "    \"\"\"\n",
    "    m = np.shape(hidden_out)[0]\n",
    "    predict_in = hidden_out * w1\n",
    "    for i in range(m):\n",
    "        predict_in[i, ] += b1\n",
    "    return predict_in\n",
    "\n",
    "#计算输出层的输出的predict_out函数\n",
    "def predict_out(predict_in):\n",
    "    \"\"\"输出层的输出\n",
    "    input:  predict_in(mat): 输出层的输入\n",
    "    output:  result(mat): 输出层的输出\n",
    "    \"\"\"\n",
    "    result = sig(predict_in)\n",
    "    return result\n",
    "\n",
    "#Sigmoid函数\n",
    "# def sig(x):\n",
    "#     \"\"\"Sigmoid激活函数\n",
    "#     input:  x(mat/float): 自变量（矩阵或者任意实数）\n",
    "#     output: Sigmoid值（mat/float）: Sigmoid函数的值\n",
    "#     \"\"\"\n",
    "#     return 1/(1+np.exp(-x))\n",
    "def sig(x):\n",
    "    x_ravel = x.ravel()  # 将numpy数组展平\n",
    "    length = len(x_ravel)\n",
    "    y = []\n",
    "    for index in range(length):\n",
    "        if x_ravel[index].any() >= 0:\n",
    "            y.append(1.0 / (1 + np.exp(-x_ravel[index])))\n",
    "        else:\n",
    "            y.append(np.exp(x_ravel[index]) / (np.exp(x_ravel[index]) + 1))\n",
    "    return np.array(y).reshape(x.shape)\n",
    "\n",
    "# def sig(x):\n",
    "#     #from numpy import exp\n",
    "#     #return 1.0/(1+exp(-inX))\n",
    "#     #优化\n",
    "#     if x.any()>=0:\n",
    "#         return 1.0/(1+exp(-x))\n",
    "#     else:\n",
    "#         return exp(x)/(1+exp(x))\n",
    "#partial_sig函数 \n",
    "def partial_sig(x):\n",
    "    m, n = np.shape(x)\n",
    "    out = np.mat(np.zeros((m, n)))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            out[i, j] = sig(x[i, j]) * (1 - sig(x[i, j]))\n",
    "    return out\n",
    "\n",
    "#计算损失函数值的get_cost函数\n",
    "def get_cost(cost):\n",
    "    m, n = np.shape(cost)\n",
    "    cost_sum = 0.0\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            cost_sum += cost[i, j] * cost[i, j]\n",
    "    return cost_sum / m\n",
    "\n",
    "#计算错误率的err_rate函数\n",
    "def err_rate(label, pre):\n",
    "    m = np.shape(label)[0]\n",
    "    err = 0.0\n",
    "    for i in range(m):\n",
    "        if label[i, 0] != pre[i, 0]:\n",
    "            err += 1\n",
    "    rate = err / m\n",
    "    return rate\n",
    "\n",
    "\n",
    "#导入数据的load_data函数\n",
    "# def load_data(filename):\n",
    "#     \"\"\"导入训练数据\n",
    "#     input：  filename(string): 文件名\n",
    "#     output:  feature_name(mat): 特征\n",
    "#             label_data(mat): 标签\n",
    "#             n_class(int): 类别的个数\n",
    "#     \"\"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "#     #1.获取特征\n",
    "#     f = open(filename) #也可以用上下文管理器进行管理文件的打开和关闭 with open(filename) as f:这样就不用f.close进行关闭\n",
    "#     feature_data = []\n",
    "#     label_tmp = []\n",
    "#     for line in f.readlines():\n",
    "#         feature_tmp = []\n",
    "#         lines = line.strip().split(\",\")\n",
    "#         for i in range(len(lines) - 1):\n",
    "#             feature_tmp.append(float(lines[i]))\n",
    "#         label_tmp.append(int(lines[-1]))  \n",
    "#         feature_data.append(feature_tmp)\n",
    "#     f.close()\n",
    "def load_data(filename_d, filename_p):\n",
    "    \"\"\"导入训练数据\n",
    "    input：  filename(string): 文件名\n",
    "    output:  feature_name(mat): 特征\n",
    "            label_data(mat): 标签\n",
    "            n_class(int): 类别的个数\n",
    "    \"\"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "    #1.获取特征\n",
    "    f = open(filename_d) #也可以用上下文管理器进行管理文件的打开和关闭 with open(filename) as f:这样就不用f.close进行关闭\n",
    "    feature_data = []\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        feature_tmp = []\n",
    "        lines = line.strip().split(\",\")\n",
    "        for i in range(len(lines) ):\n",
    "            feature_tmp.append(float(lines[i]))\n",
    "        #label_tmp.append(int(lines[-1]))  \n",
    "        feature_data.append(feature_tmp)\n",
    "    p = open(filename_p) #也可以用上下文管理器进行管理文件的打开和关闭 with open(filename) as f:这样就不用f.close进行关闭\n",
    "    label_data = []\n",
    "    \n",
    "    for line in p.readlines():\n",
    "        label_tmp = []\n",
    "        lines = line.strip().split(\",\")\n",
    "        for i in range(len(lines) ):\n",
    "            label_tmp.append(float(lines[i]))\n",
    "        #label_tmp.append(int(lines[-1]))  \n",
    "        label_data.append(label_tmp)    \n",
    "    f.close()\n",
    "    p.close()\n",
    "    #2.获取标签\n",
    "    m = len(label_tmp)\n",
    "#     #2.获取标签\n",
    "#     m = len(label_tmp)\n",
    "#     n_class = len(set(label_tmp))\n",
    "#     #label_data = np.mat(np.zeros((m , n_class)))\n",
    "#     label_data = label_tmp\n",
    "# #     for i in range(m):\n",
    "# #         label_data[i, label_tmp[i]] = 1\n",
    "    return np.mat(feature_data), np.mat(label_data), m\n",
    "\n",
    "# def load_data():\n",
    "#     dataset_iris = load_iris()\n",
    "#     data_iris = dataset_iris['data']\n",
    "#     label_tmp = dataset_iris['target']\n",
    "#     m = len(label_tmp)\n",
    "#     n_class = len(set(label_tmp))\n",
    "#     print(label_tmp)\n",
    "#     label_iris = np.mat(np.zeros((m, n_class)))\n",
    "#     for i in range(m):\n",
    "#         label_iris[i, label_tmp[i]] = 1\n",
    "#     print(label_iris)\n",
    "#     return data_iris, label_iris, n_class\n",
    "\n",
    "\n",
    "#保存bp模型的save_model函数\n",
    "def save_model(w0, w1, b0, b1):\n",
    "    def write_file(filename, source):\n",
    "        f = open(filename, \"w\")\n",
    "        m, n = np.shape(source)\n",
    "        for i in range(m):\n",
    "            tmp = []\n",
    "            for j in range(n):\n",
    "                tmp.append(str(source[i, j]))\n",
    "            f.write(\"\\t\".join(tmp) + \"\\n\")\n",
    "        f.close()\n",
    "    write_file(\"weight_w0\", w0)\n",
    "    write_file(\"weight_w1\", w1)\n",
    "    write_file(\"weight_b0\", b0)\n",
    "    write_file(\"weight_b1\", b1)\n",
    "\n",
    "#对测试样本进行预测的get_predict函数\n",
    "def get_predict(feature, w0, w1, b0, b1):\n",
    "    return predict_out(predict_in(hidden_out(hidden_in(feature, w0, b0)), w1, b1))\n",
    "\n",
    "\n",
    "#训练BP模型的主函数\n",
    "if __name__ == \"__main__\":\n",
    "    #1.导入数据\n",
    "    print(\"----------1. load Data-------------\")\n",
    "#     filename='data_t1x.csv'\n",
    "#     feature, label, n_class = load_data(filename)\n",
    "#     feature1 =  (feature-feature.mean())/feature.std()\n",
    "    filename_d='dataframe.csv'\n",
    "    filename_p='location.csv'\n",
    "    feature, label, n_class = load_data(filename_d, filename_p)\n",
    "    feature_pd = pd.DataFrame(feature)\n",
    "    label_pd = pd.DataFrame(label)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    min_max_scaler.fit(feature_pd)\n",
    "    x_train = min_max_scaler.transform(feature_pd)\n",
    "\n",
    "    min_max_scaler.fit(label_pd)\n",
    "    y_train = min_max_scaler.transform(label_pd)\n",
    "\n",
    "# 验证集归一化\n",
    "#     min_max_scaler.fit(x_valid_pd)\n",
    "#     x_valid = min_max_scaler.transform(x_valid_pd)\n",
    "\n",
    "#     min_max_scaler.fit(y_valid_pd)\n",
    "#     y_valid = min_max_scaler.transform(y_valid_pd)\n",
    "    #feature1 =  (feature-feature.mean())/feature.std()\n",
    "    #feature2 = (feature-feature.min())/feature.max()-feature.min()\n",
    "    \n",
    "    print(\"feature: \",x_train)\n",
    "    \n",
    "    #2.训练模型\n",
    "    print(\"----------2. training--------------\")\n",
    "    w0, w1, b0, b1 = bp_train(x_train, y_train,9, 5000, 0.01, n_class)\n",
    "    #3.保存模型\n",
    "    print(\"----------3. save model-------------\")\n",
    "    save_model(w0, w1, b0, b1)\n",
    "    #4.得到最终的预测结果\n",
    "    print(\"----------4. get prediction----------\")\n",
    "    y_new = get_predict(x_train, w0, w1, b0, b1)\n",
    "    #min_max_scaler.fit(result)\n",
    "    y_new = min_max_scaler.inverse_transform(y_new)\n",
    "    print(y_new)\n",
    "   # print(label)\n",
    "    #print(\"训练准确性为： \", (1 - err_rate(np.argmax(label, axis=1), np.argmax(result, axis=1))))\n",
    "   # print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74e0dc67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------1. load data--------------\n",
      "-------------2. load model--------------\n",
      "-------------3. get prediction------------\n",
      "-------------4. save result---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-52978a379ca5>:149: RuntimeWarning: overflow encountered in exp\n",
      "  y.append(1.0 / (1 + np.exp(-x_ravel[index])))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b8e3e3e610f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-------------4. save result---------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0mpre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0msave_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"result\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-b8e3e3e610f8>\u001b[0m in \u001b[0;36msave_predict\u001b[1;34m(filename, pre)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m  \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "#coding: utf-8\n",
    "import numpy as np\n",
    "#import sys\n",
    "#sys.path.append(\"F:\\Machine_Learning_Algorithm\\BP_Neural_Network\")\n",
    "#from Training_BP_NN import get_predict\n",
    "\n",
    "#生成测试样本的generate_data函数\n",
    "def generate_data():\n",
    "    \"\"\"在[-4.5, 4.5]之间随机生成20000组点\"\"\"\n",
    "    #1. 随机生成数据点\n",
    "    data = np.mat(np.zeros((200, 4)))\n",
    "    m = np.shape(data)[0]\n",
    "    x = np.mat(np.random.rand(200, 4))\n",
    "    for i in range(m):\n",
    "        data[i, 0] = x[i, 0] * 4 + 4\n",
    "        data[i, 1] = x[i, 1] * 3 + 2\n",
    "        data[i, 2] = x[i, 2] * 6 + 1\n",
    "        data[i, 3] = x[i, 3] * 3\n",
    "\n",
    "    #2. 将数据点保存到文件“test_data”中\n",
    "    f = open(\"test_data\", \"w\")\n",
    "    m, n = np.shape(data)\n",
    "    for i in range(m):\n",
    "        tmp = []\n",
    "        for j in range(n):\n",
    "            tmp.append(str(data[i, j]))\n",
    "        f.write(\"\\t\".join(tmp) + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "#导入测试数据的load_data函数\n",
    "def load_data(filename):\n",
    "    f = open(filename)\n",
    "    feature_data = []\n",
    "    for line in f.readlines():\n",
    "        feature_tmp = []\n",
    "        lines = line.strip().split(\",\")\n",
    "        for i in range(len(lines)):\n",
    "            feature_tmp.append(float(lines[i]))\n",
    "        feature_data.append(feature_tmp)\n",
    "    f.close()\n",
    "    return np.mat(feature_data)\n",
    "\n",
    "#导入BP神经网络模型的load_model函数\n",
    "def load_model(file_w0, file_w1, file_b0, file_b1):\n",
    "    def get_model(file_name):\n",
    "        f = open(file_name)\n",
    "        model = []\n",
    "        for line in f.readlines():\n",
    "            lines = line.strip().split(\"\\t\")\n",
    "            model_tmp = []\n",
    "            for x in lines:\n",
    "                model_tmp.append(float(x.strip()))\n",
    "            model.append(model_tmp)\n",
    "        f.close()\n",
    "        return np.mat(model)\n",
    "    w0 = get_model(file_w0)\n",
    "    w1 = get_model(file_w1)\n",
    "    b0 = get_model(file_b0)\n",
    "    b1 = get_model(file_b1)\n",
    "    return w0, w1, b0, b1\n",
    "\n",
    "#保存最终的预测结果的save_predict函数\n",
    "def save_predict(filename, pre):\n",
    "    f = open(filename, \"w\")\n",
    "    m = np.shape(pre)[0]\n",
    "    result = []\n",
    "    for i in range(m):\n",
    "        result.append(str(pre[i, 0]))\n",
    "    f.write(\"\\n\".join(result))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "\n",
    "#对新数据进行预测的主函数\n",
    "if __name__ == \"__main__\":\n",
    "    #generate_data()\n",
    "    #1. 导入数据\n",
    "    print(\"-------------1. load data--------------\")\n",
    "    dataTest = load_data(\"dataframe.csv\")\n",
    "    #2. 导入BP神经网络模型\n",
    "    print(\"-------------2. load model--------------\")\n",
    "    w0, w1, b0, b1 = load_model(\"weight_w0\", \"weight_w1\", \"weight_b0\", \"weight_b1\")\n",
    "    #3. 得到最终的预测值\n",
    "    print(\"-------------3. get prediction------------\")\n",
    "    result = get_predict(dataTest, w0, w1, b0, b1)\n",
    "    #. 保存最终的预测结果\n",
    "    print(\"-------------4. save result---------------\")\n",
    "    pre = np.argmax(result, axis=1)\n",
    "    save_predict(\"result\", pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b302f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21649b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce6c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6c4d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-dee64462ff95>:150: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  datas = np.loadtxt(file_data,dtype = np.float, delimiter = ',',usecols=(0,1,2,3))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  error  66.00%  loss_all 110.19\n",
      "epoch 1  error  66.00%  loss_all 110.19\n",
      "epoch 2  error  66.00%  loss_all 110.19\n",
      "epoch 3  error  66.00%  loss_all 110.11\n",
      "epoch 4  error  66.00%  loss_all 110.22\n",
      "epoch 5  error  66.00%  loss_all 110.19\n",
      "epoch 6  error  66.00%  loss_all 110.17\n",
      "epoch 7  error  66.00%  loss_all 110.15\n",
      "epoch 8  error  66.00%  loss_all 110.18\n",
      "epoch 9  error  66.00%  loss_all 110.22\n",
      "epoch 10  error  66.00%  loss_all 110.23\n",
      "epoch 11  error  66.00%  loss_all 110.13\n",
      "epoch 12  error  66.00%  loss_all 110.14\n",
      "epoch 13  error  66.00%  loss_all 110.21\n",
      "epoch 14  error  66.00%  loss_all 110.11\n",
      "epoch 15  error  66.00%  loss_all 110.21\n",
      "epoch 16  error  66.00%  loss_all 110.11\n",
      "epoch 17  error  66.00%  loss_all 110.10\n",
      "epoch 18  error  66.00%  loss_all 110.25\n",
      "epoch 19  error  66.00%  loss_all 110.15\n",
      "epoch 20  error  66.00%  loss_all 110.18\n",
      "epoch 21  error  66.00%  loss_all 110.12\n",
      "epoch 22  error  66.00%  loss_all 110.25\n",
      "epoch 23  error  66.00%  loss_all 110.22\n",
      "epoch 24  error  66.00%  loss_all 110.17\n",
      "epoch 25  error  66.00%  loss_all 110.19\n",
      "epoch 26  error  66.00%  loss_all 110.23\n",
      "epoch 27  error  66.00%  loss_all 110.19\n",
      "epoch 28  error  66.00%  loss_all 110.13\n",
      "epoch 29  error  66.00%  loss_all 110.07\n",
      "epoch 30  error  66.00%  loss_all 110.16\n",
      "epoch 31  error  66.00%  loss_all 110.23\n",
      "epoch 32  error  66.00%  loss_all 110.27\n",
      "epoch 33  error  66.00%  loss_all 110.18\n",
      "epoch 34  error  66.00%  loss_all 110.16\n",
      "epoch 35  error  66.00%  loss_all 110.14\n",
      "epoch 36  error  66.00%  loss_all 110.20\n",
      "epoch 37  error  66.00%  loss_all 110.23\n",
      "epoch 38  error  66.00%  loss_all 110.13\n",
      "epoch 39  error  66.00%  loss_all 110.09\n",
      "epoch 40  error  66.00%  loss_all 110.14\n",
      "epoch 41  error  66.00%  loss_all 110.16\n",
      "epoch 42  error  66.00%  loss_all 110.12\n",
      "epoch 43  error  66.00%  loss_all 110.26\n",
      "epoch 44  error  66.00%  loss_all 110.05\n",
      "epoch 45  error  66.00%  loss_all 110.11\n",
      "epoch 46  error  66.00%  loss_all 110.12\n",
      "epoch 47  error  66.00%  loss_all 110.18\n",
      "epoch 48  error  66.00%  loss_all 110.06\n",
      "epoch 49  error  66.00%  loss_all 110.05\n",
      "68.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmod(z):\n",
    "    h = 1./(1+np.exp(-z))\n",
    "    return h\n",
    "    \n",
    "def de_sigmoid(z,h):\n",
    "    return h*(1-h)\n",
    "    \n",
    "    \n",
    "def relu(z):\n",
    "    h = np.maximum(z, 0)\n",
    "    return h\n",
    "    \n",
    "def de_relu(z,h):\n",
    "    z[z <= 0] = 0\n",
    "    z[z > 0] = 1.0\n",
    "    return z\n",
    "    \n",
    "    \n",
    "    \n",
    "def no_active(z):\n",
    "    h = z\n",
    "    return h\n",
    "\n",
    "def de_no_active(z,h):\n",
    "    return np.ones(h.shape)\n",
    "    \n",
    "# o Nxc\n",
    "# lab Nxc    \n",
    "def loss_L2(o,lab):\n",
    "    diff = lab-o\n",
    "    sqrDiff = diff ** 2\n",
    "    return 0.5*np.sum(sqrDiff)\n",
    "    \n",
    "def de_loss_L2(o,lab):\n",
    "    return o-lab\n",
    "\n",
    "\n",
    "def loss_CE(o,lab):    \n",
    "    p = np.exp(o)/np.sum(np.exp(o),axis=1,keepdims=True)\n",
    "    loss_ce = np.sum(-lab*np.log(p))\n",
    "    return loss_ce\n",
    "\n",
    "def de_loss_CE(o,lab):\n",
    "    p = np.exp(o)/np.sum(np.exp(o),axis=1,keepdims=True)\n",
    "    return p-lab\n",
    "\n",
    "# dim_in:输入特征的维度\n",
    "# list_num_hidden： 每层输出节点的数目\n",
    "# list_act_funs： 每层的激活函数\n",
    "# list_de_act_funs: 反向传播时的函数\n",
    "\n",
    "def bulid_net(dim_in,list_num_hidden,\n",
    "              list_act_funs,list_de_act_funs):\n",
    "    layers=[]          \n",
    "    \n",
    "    # 逐层的进行网络构建\n",
    "    for i in range(len(list_num_hidden)):\n",
    "        layer = {}\n",
    "        \n",
    "        # 定义每一层的权重\n",
    "        if i ==0:\n",
    "            # layer[\"w\"]= 0.2*np.random.randn(dim_in,list_num_hidden[i])-0.1 # 用sigmoid激活函数\n",
    "            layer[\"w\"]= 0.01*np.random.randn(dim_in,list_num_hidden[i])  # 用relu 激活函数\n",
    "        else:\n",
    "            # layer[\"w\"]= 0.2*np.random.randn(list_num_hidden[i-1],list_num_hidden[i])-0.1 # 用sigmoid激活函数\n",
    "            layer[\"w\"]= 0.01*np.random.randn(list_num_hidden[i-1],list_num_hidden[i]) # 用relu 激活函数\n",
    "        \n",
    "        # 定义每一层的偏置\n",
    "        layer[\"b\"] = 0.1*np.ones([1,list_num_hidden[i]])\n",
    "        layer[\"act_fun\"]= list_act_funs[i]\n",
    "        layer[\"de_act_fun\"]= list_de_act_funs[i]\n",
    "        layers.append(layer)\n",
    "        \n",
    "    return layers\n",
    "    \n",
    "    \n",
    "# 返回每一层的输入\n",
    "# 与最后一层的输出    \n",
    "def fead_forward(datas,layers):\n",
    "    input_layers = []\n",
    "    input_acfun = []\n",
    "    for i in range(len(layers)):\n",
    "        layer = layers[i]\n",
    "        if i ==0:\n",
    "            inputs = datas\n",
    "            z = np.dot(inputs,layer[\"w\"]) + layer[\"b\"]\n",
    "            h = layer['act_fun'](z)\n",
    "            input_layers.append(inputs)\n",
    "            input_acfun.append(z)\n",
    "        else:\n",
    "            inputs = h\n",
    "            z = np.dot(inputs,layer[\"w\"])+ layer[\"b\"]\n",
    "            h = layer['act_fun'](z)\n",
    "            input_layers.append(inputs)\n",
    "            input_acfun.append(z)\n",
    "    return input_layers,input_acfun,h\n",
    "\n",
    "\n",
    "# 进行参数更新更新    \n",
    "def updata_wb(datas,labs,layers, loss_fun,de_loss_fun,alpha=0.01):\n",
    "    N,D = np.shape(datas)\n",
    "    # 进行前馈操作\n",
    "    inputs,input_acfun,output = fead_forward(datas,layers)\n",
    "    # 计算 loss\n",
    "    loss = loss_fun(output,labs)\n",
    "    #从后向前计算\n",
    "    deltas0 = de_loss_fun(output,labs)\n",
    "    # 从后向前计算误差\n",
    "    deltas =[]\n",
    "    for i in range(len(layers)):\n",
    "        index = -i-1\n",
    "        if i ==0:\n",
    "            h = output\n",
    "            z = input_acfun[index]\n",
    "            delta = deltas0*layers[index][\"de_act_fun\"](z,h)\n",
    "        else:\n",
    "            h = inputs[index+1]\n",
    "            z = input_acfun[index]\n",
    "            # print(layers[index][\"de_act_fun\"](z,h)[1])\n",
    "            delta = np.dot(delta,layers[index+1][\"w\"].T)*layers[index][\"de_act_fun\"](z,h)\n",
    "        \n",
    "        deltas.insert(0,delta)\n",
    "    \n",
    "    # 利用误差 对每一层的权重进行修成                                        \n",
    "    for i in range(len(layers)):\n",
    "        # 计算 dw 与 db\n",
    "        dw = np.dot(inputs[i].T,deltas[i])\n",
    "        db = np.sum(deltas[i],axis=0,keepdims=True)\n",
    "        # 梯度下降\n",
    "        layers[i][\"w\"] = layers[i][\"w\"] - alpha*dw\n",
    "        layers[i][\"b\"] = layers[i][\"b\"] - alpha*db\n",
    "        \n",
    "    return layers,loss\n",
    "    \n",
    "    \n",
    "def test_accuracy(datas,labs_true,layers):\n",
    "    _,_,output = fead_forward(datas,layers)\n",
    "    lab_det = np.argmax(output,axis=1)\n",
    "    labs_true = np.argmax(labs_true,axis=1) \n",
    "    N_error = np.where(np.abs(labs_true-lab_det)>0)[0].shape[0]\n",
    "   \n",
    "    error_rate = N_error/np.shape(datas)[0]\n",
    "    return error_rate\n",
    "\n",
    "    \n",
    "def load_dataset_iris(file_data,N_train):\n",
    "    # 数据读取\n",
    "    datas = np.loadtxt(file_data,dtype = np.float, delimiter = ',',usecols=(0,1,2,3))\n",
    "    labs = np.loadtxt(file_data,dtype = str, delimiter = ',',usecols=(4))\n",
    "    N,D = np.shape(datas)\n",
    "    N_test = N-N_train\n",
    "    unqiue_labs = np.unique(labs).tolist()\n",
    "    \n",
    "    dic_str2index={}\n",
    "    dic_index2str={}\n",
    "    for i in range(len(unqiue_labs)):\n",
    "        lab_str = unqiue_labs[i]\n",
    "        dic_str2index[lab_str] =i\n",
    "        dic_index2str[i]=lab_str\n",
    "    \n",
    "    labs_onehot = np.zeros([N,len(unqiue_labs)])\n",
    "    for i in range(N):\n",
    "        labs_onehot[i,dic_str2index[labs[i]]]=1\n",
    "    \n",
    "    perm = np.random.permutation(N)\n",
    "    index_train = perm[:N_train]\n",
    "    index_test = perm[N_train:]\n",
    "\n",
    "    data_train = datas[index_train,:]\n",
    "    lab_train_onehot = labs_onehot[index_train,:]\n",
    "\n",
    "    data_test = datas[index_test,:]\n",
    "    lab_test_onehot = labs_onehot[index_test]\n",
    "\n",
    "    return data_train,lab_train_onehot,data_test,lab_test_onehot,dic_index2str\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    file_data = 'iris.data'\n",
    "\n",
    "    data_train,lab_train_onehot,data_test,lab_test_onehot,dic_index2str =load_dataset_iris(file_data,100)\n",
    "    \n",
    "    N,dim_in = np.shape(data_train)\n",
    "    # 定义网络结构\n",
    "    list_num_hidden=[10,5,3]\n",
    "    list_act_funs =[relu,relu,no_active]\n",
    "    list_de_act_funs=[de_relu,de_relu,de_no_active]\n",
    "    \n",
    "    # 定义损失函数\n",
    "    loss_fun = loss_CE\n",
    "    de_loss_fun=de_loss_CE\n",
    "    \n",
    "    # loss_fun = loss_L2\n",
    "    # de_loss_fun=de_loss_L2\n",
    "    \n",
    "    layers = bulid_net(dim_in,list_num_hidden,\n",
    "          list_act_funs,list_de_act_funs)\n",
    "    \n",
    "   \n",
    "    # 进行训练\n",
    "    n_epoch = 50\n",
    "    batchsize =4    \n",
    "    N_batch = N//batchsize\n",
    "    for i in range(n_epoch):\n",
    "        # 数据打乱\n",
    "        rand_index  = np.random.permutation(N).tolist()\n",
    "        # 每个batch 更新一下weight\n",
    "        loss_sum =0\n",
    "        for j in range(N_batch):\n",
    "            index = rand_index[j*batchsize:(j+1)*batchsize]\n",
    "            batch_datas = data_train[index]\n",
    "            batch_labs = lab_train_onehot[index]\n",
    "            layers,loss = updata_wb(batch_datas,batch_labs,layers,loss_fun,de_loss_fun,alpha=0.01)\n",
    "            loss_sum = loss_sum+loss\n",
    "            \n",
    "        error = test_accuracy(data_train,lab_train_onehot,layers)\n",
    "        print(\"epoch %d  error  %.2f%%  loss_all %.2f\"%(i,error*100,loss_sum))\n",
    "    \n",
    "    #进行测试\n",
    "    error = test_accuracy(data_test,lab_test_onehot,layers)\n",
    "    print(error*100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c3cef84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------1. load Data-------------\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "----------2. training--------------\n",
      "\t------iter:  0 , cost:  0.6444749775546785\n",
      "\t------iter:  100 , cost:  0.2997280542108593\n",
      "\t------iter:  200 , cost:  0.1034931700043436\n",
      "\t------iter:  300 , cost:  0.05185958012655248\n",
      "\t------iter:  400 , cost:  0.0329475304361924\n",
      "\t------iter:  500 , cost:  0.027094518732698074\n",
      "\t------iter:  600 , cost:  0.024330592583998882\n",
      "\t------iter:  700 , cost:  0.0227178937601792\n",
      "\t------iter:  800 , cost:  0.02167715098956576\n",
      "\t------iter:  900 , cost:  0.020872475181504237\n",
      "\t------iter:  1000 , cost:  0.020215019091334087\n",
      "----------3. save model-------------\n",
      "----------4. get prediction----------\n",
      "[[9.76131532e-01 1.90404451e-02 2.78291023e-03]\n",
      " [9.73624769e-01 3.08126726e-02 2.51646573e-03]\n",
      " [9.74282076e-01 2.21064632e-02 2.72107366e-03]\n",
      " [9.66047170e-01 3.60214583e-02 2.53678190e-03]\n",
      " [9.75009434e-01 1.80255819e-02 2.81905540e-03]\n",
      " [9.70715489e-01 2.46272458e-02 2.64883891e-03]\n",
      " [9.69291286e-01 2.42094206e-02 2.69873547e-03]\n",
      " [9.72839191e-01 2.44368448e-02 2.68057962e-03]\n",
      " [9.66113764e-01 3.82279822e-02 2.50129517e-03]\n",
      " [9.71589377e-01 3.03643898e-02 2.58418496e-03]\n",
      " [9.76811232e-01 1.81597191e-02 2.80370240e-03]\n",
      " [9.65545125e-01 3.10395442e-02 2.61688626e-03]\n",
      " [9.72929676e-01 2.84718392e-02 2.59488957e-03]\n",
      " [9.74451528e-01 1.94811780e-02 2.81777565e-03]\n",
      " [9.81367586e-01 1.14845136e-02 3.01051208e-03]\n",
      " [9.77289898e-01 1.31902141e-02 2.92274035e-03]\n",
      " [9.78946387e-01 1.43415854e-02 2.89791663e-03]\n",
      " [9.75892769e-01 2.04425304e-02 2.73481090e-03]\n",
      " [9.75469697e-01 2.33255143e-02 2.67525695e-03]\n",
      " [9.72987683e-01 1.89417036e-02 2.78264337e-03]\n",
      " [9.72177492e-01 3.28032935e-02 2.52817347e-03]\n",
      " [9.72837084e-01 2.19037885e-02 2.70386251e-03]\n",
      " [9.78027418e-01 1.25133698e-02 3.01650592e-03]\n",
      " [9.63733583e-01 5.70945977e-02 2.22366298e-03]\n",
      " [9.40628456e-01 6.62577115e-02 2.41147565e-03]\n",
      " [9.68470747e-01 4.58988749e-02 2.34658527e-03]\n",
      " [9.68128888e-01 3.59976529e-02 2.47535000e-03]\n",
      " [9.75114428e-01 2.16938112e-02 2.72335795e-03]\n",
      " [9.76969521e-01 2.03043735e-02 2.73585019e-03]\n",
      " [9.63265866e-01 3.92859368e-02 2.51646100e-03]\n",
      " [9.65360677e-01 4.27121626e-02 2.44476586e-03]\n",
      " [9.76234200e-01 2.77886998e-02 2.53632893e-03]\n",
      " [9.73692178e-01 1.44239687e-02 2.91394239e-03]\n",
      " [9.78064943e-01 1.24636847e-02 2.97086355e-03]\n",
      " [9.71156100e-01 3.34975480e-02 2.51232758e-03]\n",
      " [9.78202176e-01 1.86531198e-02 2.76035095e-03]\n",
      " [9.79840890e-01 1.63124021e-02 2.82290622e-03]\n",
      " [9.74072399e-01 1.73681313e-02 2.85805264e-03]\n",
      " [9.70267193e-01 2.80618694e-02 2.63589385e-03]\n",
      " [9.74058559e-01 2.39931256e-02 2.67806618e-03]\n",
      " [9.76766512e-01 1.80429439e-02 2.79713725e-03]\n",
      " [9.64271053e-01 7.80687992e-02 1.94377658e-03]\n",
      " [9.70285456e-01 2.33999413e-02 2.73632266e-03]\n",
      " [9.65663896e-01 4.05513339e-02 2.39849340e-03]\n",
      " [9.50280443e-01 4.59459556e-02 2.43975338e-03]\n",
      " [9.72013600e-01 3.47986379e-02 2.45528337e-03]\n",
      " [9.70282859e-01 2.07104215e-02 2.76560006e-03]\n",
      " [9.70164368e-01 2.66499839e-02 2.66200274e-03]\n",
      " [9.75899103e-01 1.84119044e-02 2.80231053e-03]\n",
      " [9.75224120e-01 2.26380831e-02 2.69428142e-03]\n",
      " [2.86823632e-02 9.91811490e-01 3.60850540e-03]\n",
      " [1.96452323e-02 9.84799697e-01 6.87742649e-03]\n",
      " [1.59754358e-02 9.82538615e-01 1.20642191e-02]\n",
      " [1.15208720e-02 9.34040682e-01 5.15957758e-02]\n",
      " [1.33733021e-02 9.71610657e-01 2.30660996e-02]\n",
      " [8.16323874e-03 9.04532954e-01 5.97893964e-02]\n",
      " [1.01865055e-02 9.59905044e-01 2.14338094e-02]\n",
      " [4.86519787e-02 9.85840162e-01 3.25569436e-03]\n",
      " [2.27730636e-02 9.88614234e-01 6.44942501e-03]\n",
      " [1.02076082e-02 9.29578080e-01 3.49838074e-02]\n",
      " [2.54517504e-02 9.74397361e-01 1.33495587e-02]\n",
      " [1.59748866e-02 9.75348323e-01 1.15074134e-02]\n",
      " [3.02324731e-02 9.89116626e-01 6.05223122e-03]\n",
      " [8.46056323e-03 9.23842939e-01 5.34948730e-02]\n",
      " [4.74722919e-02 9.87841615e-01 2.51155600e-03]\n",
      " [3.23938725e-02 9.91555359e-01 3.25587253e-03]\n",
      " [4.52815231e-03 7.79704215e-01 1.35707063e-01]\n",
      " [3.26369066e-02 9.90547111e-01 3.47988129e-03]\n",
      " [5.90448410e-03 7.95111596e-01 2.63874974e-01]\n",
      " [2.84651217e-02 9.86740277e-01 5.79986635e-03]\n",
      " [1.90159510e-03 4.24428339e-01 4.78736860e-01]\n",
      " [3.49194152e-02 9.90080309e-01 3.60328745e-03]\n",
      " [3.48485087e-03 5.89402846e-01 4.51910333e-01]\n",
      " [1.26345397e-02 9.60884157e-01 2.55949970e-02]\n",
      " [2.99514250e-02 9.90619329e-01 4.02576377e-03]\n",
      " [2.82107041e-02 9.90414720e-01 4.30725883e-03]\n",
      " [1.59114814e-02 9.80675956e-01 1.61533257e-02]\n",
      " [5.48486275e-03 8.68669897e-01 1.28566167e-01]\n",
      " [8.65204143e-03 9.32120652e-01 4.59805235e-02]\n",
      " [7.64433327e-02 9.90597035e-01 1.62691227e-03]\n",
      " [2.86003734e-02 9.85539380e-01 6.50639272e-03]\n",
      " [3.93407292e-02 9.89540176e-01 3.62414245e-03]\n",
      " [3.41702766e-02 9.89242775e-01 3.83773312e-03]\n",
      " [1.13981936e-03 1.38940161e-01 8.61113074e-01]\n",
      " [3.08208296e-03 5.92850933e-01 2.67473157e-01]\n",
      " [1.08507481e-02 9.61348761e-01 1.59921867e-02]\n",
      " [1.79526852e-02 9.84516679e-01 9.19139024e-03]\n",
      " [1.45448123e-02 9.67381976e-01 3.34591320e-02]\n",
      " [2.05050927e-02 9.80589392e-01 7.02330090e-03]\n",
      " [1.41073417e-02 9.59052252e-01 2.52925445e-02]\n",
      " [7.79438826e-03 8.75119360e-01 8.31189390e-02]\n",
      " [1.17406707e-02 9.61514199e-01 2.26476939e-02]\n",
      " [2.68175203e-02 9.86707109e-01 6.14110636e-03]\n",
      " [5.01996225e-02 9.86477821e-01 3.37810249e-03]\n",
      " [1.27016281e-02 9.56689393e-01 2.47877404e-02]\n",
      " [2.33570288e-02 9.84763677e-01 5.44772652e-03]\n",
      " [1.76333095e-02 9.77045624e-01 1.02186197e-02]\n",
      " [2.53335564e-02 9.88249913e-01 5.31197815e-03]\n",
      " [1.12042838e-01 9.82322659e-01 1.71603783e-03]\n",
      " [1.96775070e-02 9.79869430e-01 9.19549371e-03]\n",
      " [4.04795730e-04 2.38305196e-02 9.85218632e-01]\n",
      " [5.93350512e-04 4.60394373e-02 9.61394436e-01]\n",
      " [5.96556217e-04 5.26903283e-02 9.63134059e-01]\n",
      " [6.11852974e-04 5.24405483e-02 9.55128251e-01]\n",
      " [4.44740355e-04 3.06982938e-02 9.79137438e-01]\n",
      " [4.24886499e-04 3.76298496e-02 9.77490005e-01]\n",
      " [6.99664592e-04 5.12502927e-02 9.46596482e-01]\n",
      " [6.04805762e-04 6.39174661e-02 9.56371711e-01]\n",
      " [5.13004474e-04 5.42551539e-02 9.65739246e-01]\n",
      " [5.24116411e-04 3.82280085e-02 9.71875606e-01]\n",
      " [1.49252664e-03 3.35301646e-01 6.52916690e-01]\n",
      " [7.42183033e-04 7.32948926e-02 9.42990053e-01]\n",
      " [7.52824025e-04 7.95782796e-02 9.37754247e-01]\n",
      " [5.00798007e-04 3.72660943e-02 9.72921224e-01]\n",
      " [4.58282478e-04 2.78427275e-02 9.81209206e-01]\n",
      " [6.42506608e-04 5.58908160e-02 9.52770846e-01]\n",
      " [9.15725779e-04 1.03199720e-01 9.02571575e-01]\n",
      " [5.75770503e-04 4.74862391e-02 9.60703803e-01]\n",
      " [2.87936538e-04 3.24736210e-02 9.83533783e-01]\n",
      " [1.10166816e-03 1.45686787e-01 8.89801471e-01]\n",
      " [5.81801792e-04 4.81016443e-02 9.64052868e-01]\n",
      " [6.32870802e-04 4.61854759e-02 9.58253004e-01]\n",
      " [4.07232623e-04 4.33002262e-02 9.75807576e-01]\n",
      " [1.72180462e-03 3.04132388e-01 7.24795330e-01]\n",
      " [6.76968358e-04 6.14101295e-02 9.45207728e-01]\n",
      " [1.11947087e-03 1.52616715e-01 8.63800122e-01]\n",
      " [2.03171875e-03 4.07385851e-01 5.97739811e-01]\n",
      " [1.67935965e-03 3.25129959e-01 6.41024070e-01]\n",
      " [4.68792009e-04 3.50253671e-02 9.76003727e-01]\n",
      " [2.58455012e-03 4.66782941e-01 5.76247735e-01]\n",
      " [7.29252737e-04 8.07124696e-02 9.48493130e-01]\n",
      " [1.57412803e-03 3.64906717e-01 6.38811423e-01]\n",
      " [4.42546441e-04 3.16053086e-02 9.79292437e-01]\n",
      " [2.88202502e-03 5.16006505e-01 4.68064661e-01]\n",
      " [7.07170785e-04 8.17786883e-02 9.27547730e-01]\n",
      " [6.06950932e-04 5.64185221e-02 9.66125180e-01]\n",
      " [5.32107252e-04 3.33672598e-02 9.73788558e-01]\n",
      " [8.93942400e-04 9.70655511e-02 9.00948638e-01]\n",
      " [1.78699940e-03 3.62452769e-01 5.90952722e-01]\n",
      " [1.06002361e-03 1.71743659e-01 8.55681811e-01]\n",
      " [5.06801763e-04 3.56073808e-02 9.75320584e-01]\n",
      " [1.17711679e-03 2.71211800e-01 7.78117416e-01]\n",
      " [5.93350512e-04 4.60394373e-02 9.61394436e-01]\n",
      " [4.82203533e-04 3.29787395e-02 9.76899032e-01]\n",
      " [4.97401492e-04 3.27547100e-02 9.76988346e-01]\n",
      " [7.55097924e-04 8.98016879e-02 9.31143438e-01]\n",
      " [9.16873955e-04 1.04292914e-01 9.22457742e-01]\n",
      " [1.02723167e-03 1.45808887e-01 8.67468227e-01]\n",
      " [6.26535519e-04 4.66341134e-02 9.57626965e-01]\n",
      " [9.13208799e-04 9.49192677e-02 8.94605646e-01]]\n",
      "训练准确性为：  0.98\n"
     ]
    }
   ],
   "source": [
    "##coding: utf-8\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn.datasets import load_iris\n",
    "from numpy import *\n",
    "\n",
    "\n",
    "\n",
    "#BP神经网络模型的训练\n",
    "def bp_train(feature, label, n_hidden, maxCycle, alpha, n_output):\n",
    "    \"\"\"计算隐含层的输入\n",
    "    input:  feature(mat): 特征\n",
    "            label(mat):  标签\n",
    "            n_hidden(int): 隐含层的节点数\n",
    "            maxCycle(int):  最大的迭代次数\n",
    "            alpha(float):  学习率\n",
    "            n_output(int):  输出层的节点数\n",
    "    output:  w0(mat):  输入层到隐含层之间的的权重\n",
    "             b0(mat):  输入层到隐含层之间的偏置\n",
    "             w1(mat):  隐含层到输出层之间的权重\n",
    "             b1(mat):  隐含层到输出层之间的偏置\n",
    "    \"\"\"\n",
    "    m, n = np.shape(feature)\n",
    "    #1. 随机初始化参数（权重，偏置， 网络层结构， 激活函数）\n",
    "    w0 = np.mat(np.random.rand(n, n_hidden))\n",
    "    w0 = w0 * (8.0 * sqrt(6) / sqrt(n + n_hidden)) -\\\n",
    "         np.mat(np.ones((n, n_hidden))) * (4.0 * sqrt(6) / sqrt(n + n_hidden))\n",
    "    b0 = np.mat(np.random.rand(1, n_hidden))\n",
    "    b0 = b0 * (8.0 * sqrt(6) / sqrt(n + n_hidden)) -\\\n",
    "         np.mat(np.ones((1, n_hidden))) * (4.0 * sqrt(6) / sqrt(n + n_hidden))\n",
    "    w1 = np.mat(np.random.rand(n_hidden, n_output))\n",
    "    w1 = w1 * (8.0 * sqrt(6) / sqrt(n_hidden + n_output)) -\\\n",
    "         np.mat(np.ones((n_hidden, n_output))) * (4.0 * sqrt(6) / sqrt(n_hidden + n_output))\n",
    "    b1 = np.mat(np.random.rand(1, n_output))\n",
    "    b1 = b1 * (8.0 * sqrt(6) / sqrt(n_hidden + n_output)) -\\\n",
    "         np.mat(np.ones((1, n_output))) * (4.0 * sqrt(6) / sqrt(n_hidden + n_output))\n",
    "\n",
    "    #2. 训练\n",
    "    i = 0\n",
    "    while i <= maxCycle:\n",
    "        #信号正向传播\n",
    "        #计算隐含层的输入\n",
    "        hidden_input = hidden_in(feature, w0, b0)\n",
    "        #计算隐含层的输出\n",
    "        hidden_output = hidden_out(hidden_input)\n",
    "        #计算输出层的输入\n",
    "        output_in = predict_in(hidden_output, w1, b1)\n",
    "        #计算输出层的输出\n",
    "        output_out = predict_out(output_in)\n",
    "\n",
    "        #误差的反向传播\n",
    "        #隐含层到输出层之间的残差\n",
    "        delta_output = -np.multiply((label - output_out), partial_sig(output_in))\n",
    "        #输入层到隐含层之间的残差\n",
    "        delta_hidden = np.multiply((delta_output * w1.T), partial_sig(hidden_input))\n",
    "\n",
    "        #修正权重和偏置\n",
    "        w1 = w1 - alpha * (hidden_output.T * delta_output)\n",
    "        b1 = b1 - alpha * np.sum(delta_output, axis=0) * (1.0 / m)\n",
    "        w0 = w0 - alpha * (feature.T * delta_hidden)\n",
    "        b0 = b0 - alpha * np.sum(delta_hidden, axis=0) * (1.0 / m)\n",
    "        if i % 100 == 0:\n",
    "            print(\"\\t------iter: \", i, \", cost: \", (1.0/2) * get_cost(get_predict(feature, w0, w1, b0, b1) - label))\n",
    "        i += 1\n",
    "    return w0, w1, b0, b1\n",
    "\n",
    "#计算隐含层的输入的hidden_in函数\n",
    "def hidden_in(feature, w0, b0):\n",
    "    \"\"\"隐含层的输入\n",
    "    input:  feature(mat): 特征\n",
    "            w0(mat): 输入层到隐含层之间的权重\n",
    "            b0(mat): 输入层到隐含层之间的偏置\n",
    "    output:  hidden_in(mat): 隐含层的输入\n",
    "    \"\"\"\n",
    "    m = np.shape(feature)[0]\n",
    "    hidden_in = feature * w0\n",
    "    for i in range(m):\n",
    "        hidden_in[i, ] += b0\n",
    "    return hidden_in\n",
    "\n",
    "#计算隐含层的输出的hidden_out函数\n",
    "def hidden_out(hidden_in):\n",
    "    \"\"\"隐含层的输出\n",
    "    input:  hidden_in(mat): 隐含层的输入\n",
    "    output: hidden_output(mat): 隐含层的输出\n",
    "    \"\"\"\n",
    "    hidden_output = sig(hidden_in)\n",
    "    return hidden_output\n",
    "\n",
    "#计算输出层的输入的predict_in函数\n",
    "def predict_in(hidden_out, w1, b1):\n",
    "    \"\"\"输出层的输入\n",
    "    input:  hidden_out(mat): 隐含层的输出\n",
    "            w1(mat): 隐含层到输出层之间的权重\n",
    "            b1(mat): 隐含层到输出层之间的偏置\n",
    "    output:  predict_in(mat): 输出层的输入\n",
    "    \"\"\"\n",
    "    m = np.shape(hidden_out)[0]\n",
    "    predict_in = hidden_out * w1\n",
    "    for i in range(m):\n",
    "        predict_in[i, ] += b1\n",
    "    return predict_in\n",
    "\n",
    "#计算输出层的输出的predict_out函数\n",
    "def predict_out(predict_in):\n",
    "    \"\"\"输出层的输出\n",
    "    input:  predict_in(mat): 输出层的输入\n",
    "    output:  result(mat): 输出层的输出\n",
    "    \"\"\"\n",
    "    result = sig(predict_in)\n",
    "    return result\n",
    "\n",
    "#Sigmoid函数\n",
    "def sig(x):\n",
    "    \"\"\"Sigmoid激活函数\n",
    "    input:  x(mat/float): 自变量（矩阵或者任意实数）\n",
    "    output: Sigmoid值（mat/float）: Sigmoid函数的值\n",
    "    \"\"\"\n",
    "    return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "#partial_sig函数\n",
    "def partial_sig(x):\n",
    "    m, n = np.shape(x)\n",
    "    out = np.mat(np.zeros((m, n)))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            out[i, j] = sig(x[i, j]) * (1 - sig(x[i, j]))\n",
    "    return out\n",
    "\n",
    "#计算损失函数值的get_cost函数\n",
    "def get_cost(cost):\n",
    "    m, n = np.shape(cost)\n",
    "    cost_sum = 0.0\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            cost_sum += cost[i, j] * cost[i, j]\n",
    "    return cost_sum / m\n",
    "\n",
    "#计算错误率的err_rate函数\n",
    "def err_rate(label, pre):\n",
    "    m = np.shape(label)[0]\n",
    "    err = 0.0\n",
    "    for i in range(m):\n",
    "        if label[i, 0] != pre[i, 0]:\n",
    "            err += 1\n",
    "    rate = err / m\n",
    "    return rate\n",
    "\n",
    "\n",
    "# #导入数据的load_data函数\n",
    "# def load_data(filename):\n",
    "#     \"\"\"导入训练数据\n",
    "#     input：  filename(string): 文件名\n",
    "#     output:  feature_name(mat): 特征\n",
    "#             label_data(mat): 标签\n",
    "#             n_class(int): 类别的个数\n",
    "#     \"\"\"\n",
    "#     #1.获取特征\n",
    "#     f = open(filename) #也可以用上下文管理器进行管理文件的打开和关闭 with open(filename) as f:这样就不用f.close进行关闭\n",
    "#     feature_data = []\n",
    "#     label_tmp = []\n",
    "#     for line in f.readlines():\n",
    "#         feature_tmp = []\n",
    "#         lines = line.strip().split(\"\\t\")\n",
    "#         for i in range(len(lines) - 1):\n",
    "#             feature_tmp.append(float(lines[i]))\n",
    "#         label_tmp.append(int(lines[-1]))\n",
    "#         feature_data.append(feature_tmp)\n",
    "#    # f.close()\n",
    "#\n",
    "#     #2.获取标签\n",
    "#     m = len(label_tmp)\n",
    "#     n_class = len(set(label_tmp))\n",
    "#     label_data = np.mat(np.zeros((m , n_class)))\n",
    "#     for i in range(m):\n",
    "#         label_data[i, label_tmp[i]] = 1\n",
    "#     return np.mat(feature_data), label_data, n_class\n",
    "\n",
    "def load_data():\n",
    "    dataset_iris = load_iris()\n",
    "    data_iris = dataset_iris['data']\n",
    "    label_tmp = dataset_iris['target']\n",
    "    m = len(label_tmp)\n",
    "    n_class = len(set(label_tmp))\n",
    "    print(label_tmp)\n",
    "    label_iris = np.mat(np.zeros((m, n_class)))\n",
    "    for i in range(m):\n",
    "        label_iris[i, label_tmp[i]] = 1\n",
    "    print(label_iris)\n",
    "    return data_iris, label_iris, n_class\n",
    "\n",
    "\n",
    "#保存bp模型的save_model函数\n",
    "def save_model(w0, w1, b0, b1):\n",
    "    def write_file(filename, source):\n",
    "        f = open(filename, \"w\")\n",
    "        m, n = np.shape(source)\n",
    "        for i in range(m):\n",
    "            tmp = []\n",
    "            for j in range(n):\n",
    "                tmp.append(str(source[i, j]))\n",
    "            f.write(\"\\t\".join(tmp) + \"\\n\")\n",
    "        f.close()\n",
    "    write_file(\"weight_w0\", w0)\n",
    "    write_file(\"weight_w1\", w1)\n",
    "    write_file(\"weight_b0\", b0)\n",
    "    write_file(\"weight_b1\", b1)\n",
    "\n",
    "#对测试样本进行预测的get_predict函数\n",
    "def get_predict(feature, w0, w1, b0, b1):\n",
    "    return predict_out(predict_in(hidden_out(hidden_in(feature, w0, b0)), w1, b1))\n",
    "\n",
    "import copy \n",
    "import mglearn\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#训练BP模型的主函数\n",
    "if __name__ == \"__main__\":\n",
    "    #1.导入数据\n",
    "    print(\"----------1. load Data-------------\")\n",
    "    feature, label, n_class = load_data()\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = (16.0, 16.0)\n",
    "    temp = copy.deepcopy(corr_names[arg_indexs[0]])\n",
    "    temp = temp[len(temp)-5:-1]\n",
    "    temp.append(corr_matrix.columns[arg_indexs[0]])\n",
    "    grr = pd.plotting.scatter_matrix(feature[temp],marker='o',c = train_label.iloc[:,2],hist_kwds={'bins':20})#,cmap=mglearn.cm3)\n",
    "    plt.savefig(\"imgs/第1题强相关变量之间的散点图1.png\",dpi=300)\n",
    "    #2.训练模型\n",
    "    print(\"----------2. training--------------\")\n",
    "    w0, w1, b0, b1 = bp_train(feature, label, 30, 1000, 0.01, n_class)\n",
    "    #3.保存模型\n",
    "    print(\"----------3. save model-------------\")\n",
    "    save_model(w0, w1, b0, b1)\n",
    "    #4.得到最终的预测结果\n",
    "    print(\"----------4. get prediction----------\")\n",
    "    result = get_predict(feature, w0, w1, b0, b1)\n",
    "    print(result)\n",
    "    print(\"训练准确性为： \", (1 - err_rate(np.argmax(label, axis=1), np.argmax(result, axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ‘"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
