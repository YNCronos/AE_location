{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad96c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import boston_housing\n",
    "from keras.layers import Dense, Dropout\n",
    "#from keras.utils import multi_gpu_model\n",
    "from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "\n",
    "from keras import regularizers  # 正则化\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28053f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "#from sklearn.datasets import load_iris\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c84ecd21",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------1. load Data-------------\n",
      "----------2. training--------------\n",
      "\t------iter:  0 , cost:  527.2956000830549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-04ea484fb6da>:76: RuntimeWarning: overflow encountered in multiply\n",
      "  delta_output = -np.multiply((label - output_out), partial_sig(output_in))\n",
      "<ipython-input-20-04ea484fb6da>:78: RuntimeWarning: invalid value encountered in multiply\n",
      "  delta_hidden = np.multiply((delta_output * w1.T), partial_sig(hidden_input))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t------iter:  100 , cost:  nan\n",
      "\t------iter:  200 , cost:  nan\n",
      "\t------iter:  300 , cost:  nan\n",
      "\t------iter:  400 , cost:  nan\n",
      "\t------iter:  500 , cost:  nan\n",
      "\t------iter:  600 , cost:  nan\n",
      "\t------iter:  700 , cost:  nan\n",
      "\t------iter:  800 , cost:  nan\n",
      "\t------iter:  900 , cost:  nan\n",
      "\t------iter:  1000 , cost:  nan\n",
      "----------3. save model-------------\n",
      "----------4. get prediction----------\n",
      "[[nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]\n",
      " [nan nan]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#BP神经网络模型的训练\n",
    "def bp_train(feature, label, n_hidden, maxCycle, alpha, n_output):\n",
    "    \"\"\"计算隐含层的输入\n",
    "        input:  feature(mat): 特征\n",
    "                label(mat):  标签\n",
    "                n_hidden(int): 隐含层的节点数\n",
    "                maxCycle(int):  最大的迭代次数\n",
    "                alpha(float):  学习率\n",
    "                n_output(int):  输出层的节点数\n",
    "        output:  w0(mat):  输入层到隐含层之间的的权重\n",
    "                 b0(mat):  输入层到隐含层之间的偏置\n",
    "                 w1(mat):  隐含层到输出层之间的权重\n",
    "                 b1(mat):  隐含层到输出层之间的偏置\n",
    "    \"\"\"\n",
    "    m, n = np.shape(feature)\n",
    "    #1. 随机初始化参数（权重，偏置， 网络层结构， 激活函数）\n",
    "    w0 = np.mat(np.random.rand(n, n_hidden))\n",
    "    w0 = w0 * (8.0 * sqrt(6) / sqrt(n + n_hidden)) -\\\n",
    "         np.mat(np.ones((n, n_hidden))) * (4.0 * sqrt(6) / sqrt(n + n_hidden))\n",
    "    b0 = np.mat(np.random.rand(1, n_hidden))\n",
    "    b0 = b0 * (8.0 * sqrt(6) / sqrt(n + n_hidden)) -\\\n",
    "         np.mat(np.ones((1, n_hidden))) * (4.0 * sqrt(6) / sqrt(n + n_hidden))\n",
    "    w1 = np.mat(np.random.rand(n_hidden, n_output))\n",
    "    w1 = w1 * (8.0 * sqrt(6) / sqrt(n_hidden + n_output)) -\\\n",
    "         np.mat(np.ones((n_hidden, n_output))) * (4.0 * sqrt(6) / sqrt(n_hidden + n_output))\n",
    "    b1 = np.mat(np.random.rand(1, n_output))\n",
    "    b1 = b1 * (8.0 * sqrt(6) / sqrt(n_hidden + n_output)) -\\\n",
    "         np.mat(np.ones((1, n_output))) * (4.0 * sqrt(6) / sqrt(n_hidden + n_output))\n",
    "\n",
    "    #2. 训练\n",
    "    i = 0\n",
    "    while i <= maxCycle:\n",
    "        #信号正向传播\n",
    "        #计算隐含层的输入\n",
    "        hidden_input = hidden_in(feature, w0, b0)\n",
    "        #计算隐含层的输出\n",
    "        hidden_output = hidden_out(hidden_input)\n",
    "        #计算输出层的输入\n",
    "        output_in = predict_in(hidden_output, w1, b1)\n",
    "        #计算输出层的输出\n",
    "        output_out = predict_out(output_in)\n",
    "\n",
    "        #误差的反向传播\n",
    "        #隐含层到输出层之间的残差\n",
    "        delta_output = -np.multiply((label - output_out), partial_sig(output_in))\n",
    "        #输入层到隐含层之间的残差\n",
    "        delta_hidden = np.multiply((delta_output * w1.T), partial_sig(hidden_input))\n",
    "\n",
    "        #修正权重和偏置\n",
    "        w1 = w1 - alpha * (hidden_output.T * delta_output)\n",
    "        b1 = b1 - alpha * np.sum(delta_output, axis=0) * (1.0 / m)\n",
    "        w0 = w0 - alpha * (feature.T * delta_hidden) \n",
    "        b0 = b0 - alpha * np.sum(delta_hidden, axis=0) * (1.0 / m)\n",
    "        if i % 100 == 0:\n",
    "            print(\"\\t------iter: \", i, \", cost: \", (1.0/2) * get_cost(get_predict(feature, w0, w1, b0, b1) - label))\n",
    "        i += 1\n",
    "    return w0, w1, b0, b1\n",
    "\n",
    "#计算隐含层的输入的hidden_in函数\n",
    "def hidden_in(feature, w0, b0):\n",
    "    \"\"\"隐含层的输入\n",
    "    input:  feature(mat): 特征\n",
    "            w0(mat): 输入层到隐含层之间的权重\n",
    "            b0(mat): 输入层到隐含层之间的偏置\n",
    "    output:  hidden_in(mat): 隐含层的输入\n",
    "    \"\"\"\n",
    "    m = np.shape(feature)[0]\n",
    "    hidden_in = feature * w0\n",
    "    for i in range(m):\n",
    "        hidden_in[i, ] += b0\n",
    "    return hidden_in\n",
    "\n",
    "#计算隐含层的输出的hidden_out函数\n",
    "def hidden_out(hidden_in):\n",
    "    \"\"\"隐含层的输出\n",
    "    input:  hidden_in(mat): 隐含层的输入\n",
    "    output: hidden_output(mat): 隐含层的输出\n",
    "    \"\"\"\n",
    "    hidden_output = sig(hidden_in)\n",
    "    return hidden_output\n",
    "\n",
    "#计算输出层的输入的predict_in函数\n",
    "def predict_in(hidden_out, w1, b1):\n",
    "    \"\"\"输出层的输入\n",
    "    input:  hidden_out(mat): 隐含层的输出\n",
    "            w1(mat): 隐含层到输出层之间的权重\n",
    "            b1(mat): 隐含层到输出层之间的偏置\n",
    "    output:  predict_in(mat): 输出层的输入\n",
    "    \"\"\"\n",
    "    m = np.shape(hidden_out)[0]\n",
    "    predict_in = hidden_out * w1\n",
    "    for i in range(m):\n",
    "        predict_in[i, ] += b1\n",
    "    return predict_in\n",
    "\n",
    "#计算输出层的输出的predict_out函数\n",
    "def predict_out(predict_in):\n",
    "    \"\"\"输出层的输出\n",
    "    input:  predict_in(mat): 输出层的输入\n",
    "    output:  result(mat): 输出层的输出\n",
    "    \"\"\"\n",
    "    result = sig(predict_in)\n",
    "    return result\n",
    "\n",
    "\n",
    "#导入数据的load_data函数\n",
    "# def load_data(filename):\n",
    "#     \"\"\"导入训练数据\n",
    "#     input：  filename(string): 文件名\n",
    "#     output:  feature_name(mat): 特征\n",
    "#             label_data(mat): 标签\n",
    "#             n_class(int): 类别的个数\n",
    "#     \"\"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "#     #1.获取特征\n",
    "#     f = open(filename) #也可以用上下文管理器进行管理文件的打开和关闭 with open(filename) as f:这样就不用f.close进行关闭\n",
    "#     feature_data = []\n",
    "#     label_tmp = []\n",
    "#     for line in f.readlines():\n",
    "#         feature_tmp = []\n",
    "#         lines = line.strip().split(\",\")\n",
    "#         for i in range(len(lines) - 1):\n",
    "#             feature_tmp.append(float(lines[i]))\n",
    "#         label_tmp.append(int(lines[-1]))  \n",
    "#         feature_data.append(feature_tmp)\n",
    "#     f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dc2293c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------1. load Data-------------\n",
      "feature:         x     y\n",
      "0   19.0  10.5\n",
      "1   17.8  10.8\n",
      "2   17.2  11.2\n",
      "3   18.9  12.4\n",
      "4   17.8  12.5\n",
      "..   ...   ...\n",
      "60  17.5   6.0\n",
      "61  21.6   7.6\n",
      "62  23.2   7.3\n",
      "63  26.5   6.7\n",
      "64  29.3   6.8\n",
      "\n",
      "[65 rows x 2 columns]\n",
      "----------2. training--------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to coerce to DataFrame, shape must be (65, 2): given (65, 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-216754691113>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;31m#2.训练模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"----------2. training--------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m     \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbp_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m     \u001b[1;31m#3.保存模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"----------3. save model-------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-216754691113>\u001b[0m in \u001b[0;36mbp_train\u001b[1;34m(feature, label, n_hidden, maxCycle, alpha, n_output)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m#误差的反向传播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m#隐含层到输出层之间的残差\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mdelta_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moutput_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartial_sig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[1;31m#输入层到隐含层之间的残差\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mdelta_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_output\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartial_sig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\pandas\\core\\arraylike.py\u001b[0m in \u001b[0;36m__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__sub__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__rsub__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5978\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m  \u001b[1;31m# only relevant for Series other case\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5980\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malign_method_FRAME\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5982\u001b[0m         \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch_frame_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36malign_method_FRAME\u001b[1;34m(left, right, axis, flex, level)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    245\u001b[0m                     \u001b[1;34m\"Unable to coerce to DataFrame, shape \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m                     \u001b[1;34mf\"must be {left.shape}: given {right.shape}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to coerce to DataFrame, shape must be (65, 2): given (65, 65)"
     ]
    }
   ],
   "source": [
    "\n",
    "#Sigmoid函数\n",
    "def relu(x):\n",
    "    \"\"\"Sigmoid激活函数\n",
    "    input:  x(mat/float): 自变量（矩阵或者任意实数）\n",
    "    output: Sigmoid值（mat/float）: Sigmoid函数的值\n",
    "    \"\"\"\n",
    "    return maximum(0,x)\n",
    "# def sig(x):\n",
    "#     x_ravel = x.ravel()  # 将numpy数组展平\n",
    "#     length = len(x_ravel)\n",
    "#     y = []\n",
    "#     for index in range(length):\n",
    "#         if x_ravel[index].any() >= 0:\n",
    "#             y.append(1.0 / (1 + np.exp(-x_ravel[index])))\n",
    "#         else:\n",
    "#             y.append(np.exp(x_ravel[index]) / (np.exp(x_ravel[index]) + 1))\n",
    "#     return np.array(y).reshape(x.shape)\n",
    "\n",
    "# def sig(x):\n",
    "#     #from numpy import exp\n",
    "#     #return 1.0/(1+exp(-inX))\n",
    "#     #优化\n",
    "#     if x.any()>=0:\n",
    "#         return 1.0/(1+exp(-x))\n",
    "#     else:\n",
    "#         return exp(x)/(1+exp(x))\n",
    "#partial_sig函数 \n",
    "def partial_sig(x):\n",
    "    m, n = np.shape(x)\n",
    "    out = np.mat(np.zeros((m, n)))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            out[i, j] = sig(x[i, j]) * (1 - sig(x[i, j]))\n",
    "    return out\n",
    "\n",
    "#计算损失函数值的get_cost函数\n",
    "def get_cost(cost):\n",
    "    m, n = np.shape(cost)\n",
    "    cost_sum = 0.0\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            cost_sum += cost[i, j] * cost[i, j]\n",
    "    return cost_sum / m\n",
    "\n",
    "#计算错误率的err_rate函数\n",
    "def err_rate(label, pre):\n",
    "    m = np.shape(label)[0]\n",
    "    err = 0.0\n",
    "    for i in range(m):\n",
    "        if label[i, 0] != pre[i, 0]:\n",
    "            err += 1\n",
    "    rate = err / m\n",
    "    return rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c4f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename_d, filename_p):\n",
    "    \"\"\"导入训练数据\n",
    "    input：  filename(string): 文件名\n",
    "    output:  feature_name(mat): 特征\n",
    "            label_data(mat): 标签\n",
    "            n_class(int): 类别的个数\n",
    "    \"\"\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "    #1.获取特征\n",
    "    f = open(filename_d) #也可以用上下文管理器进行管理文件的打开和关闭 with open(filename) as f:这样就不用f.close进行关闭\n",
    "    feature_data = []\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        feature_tmp = []\n",
    "        lines = line.strip().split(\",\")\n",
    "        for i in range(len(lines) ):\n",
    "            feature_tmp.append(float(lines[i]))\n",
    "        #label_tmp.append(int(lines[-1]))  \n",
    "        feature_data.append(feature_tmp)\n",
    "    p = open(filename_p) #也可以用上下文管理器进行管理文件的打开和关闭 with open(filename) as f:这样就不用f.close进行关闭\n",
    "    label_data = []\n",
    "    \n",
    "    for line in p.readlines():\n",
    "        label_tmp = []\n",
    "        lines = line.strip().split(\",\")\n",
    "        for i in range(len(lines) ):\n",
    "            label_tmp.append(float(lines[i]))\n",
    "        #label_tmp.append(int(lines[-1]))  \n",
    "        label_data.append(label_tmp)    \n",
    "    f.close()\n",
    "    p.close()\n",
    "    #2.获取标签\n",
    "    m = len(label_tmp)\n",
    "#     #2.获取标签\n",
    "#     m = len(label_tmp)\n",
    "#     n_class = len(set(label_tmp))\n",
    "#     #label_data = np.mat(np.zeros((m , n_class)))\n",
    "#     label_data = label_tmp\n",
    "# #     for i in range(m):\n",
    "# #         label_data[i, label_tmp[i]] = 1\n",
    "    return np.mat(feature_data), label_data, m\n",
    "\n",
    "# def load_data():\n",
    "#     dataset_iris = load_iris()\n",
    "#     data_iris = dataset_iris['data']\n",
    "#     label_tmp = dataset_iris['target']\n",
    "#     m = len(label_tmp)\n",
    "#     n_class = len(set(label_tmp))\n",
    "#     print(label_tmp)\n",
    "#     label_iris = np.mat(np.zeros((m, n_class)))\n",
    "#     for i in range(m):\n",
    "#         label_iris[i, label_tmp[i]] = 1\n",
    "#     print(label_iris)\n",
    "#     return data_iris, label_iris, n_class\n",
    "\n",
    "\n",
    "#保存bp模型的save_model函数\n",
    "def save_model(w0, w1, b0, b1):\n",
    "    def write_file(filename, source):\n",
    "        f = open(filename, \"w\")\n",
    "        m, n = np.shape(source)\n",
    "        for i in range(m):\n",
    "            tmp = []\n",
    "            for j in range(n):\n",
    "                tmp.append(str(source[i, j]))\n",
    "            f.write(\"\\t\".join(tmp) + \"\\n\")\n",
    "        f.close()\n",
    "    write_file(\"weight_w0\", w0)\n",
    "    write_file(\"weight_w1\", w1)\n",
    "    write_file(\"weight_b0\", b0)\n",
    "    write_file(\"weight_b1\", b1)\n",
    "\n",
    "#对测试样本进行预测的get_predict函数\n",
    "def get_predict(feature, w0, w1, b0, b1):\n",
    "    return predict_out(predict_in(hidden_out(hidden_in(feature, w0, b0)), w1, b1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f190a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练BP模型的主函数\n",
    "if __name__ == \"__main__\":\n",
    "    #1.导入数据\n",
    "    print(\"----------1. load Data-------------\")\n",
    "#     filename='data_t1x.csv'\n",
    "#     feature, label, n_class = load_data(filename)\n",
    "#     feature1 =  (feature-feature.mean())/feature.std()\n",
    "    filename_d='dataframe.csv'\n",
    "    filename_p='location.csv'\n",
    "    feature, label, n_class = load_data(filename_d, filename_p)\n",
    "    feature1 =  (feature-feature.mean())/feature.std()\n",
    "    feature2 = (feature1)\n",
    "   # print(\"feature: \",feature1)\n",
    "    \n",
    "    #2.训练模型\n",
    "    print(\"----------2. training--------------\")\n",
    "    w0, w1, b0, b1 = bp_train(feature1, label, 15, 1000, 0.001, n_class)\n",
    "    #3.保存模型\n",
    "    print(\"----------3. save model-------------\")\n",
    "    save_model(w0, w1, b0, b1)\n",
    "    #4.得到最终的预测结果\n",
    "    print(\"----------4. get prediction----------\")\n",
    "    result = get_predict(feature1, w0, w1, b0, b1)\n",
    "   # print(\"训练准确性为： \", (1 - err_rate(np.argmax(label, axis=1), np.argmax(result, axis=1))))\n",
    "    print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75a3c80b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dot product shape mismatch, (65, 2) vs (1, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2b2214cbfc73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[0mb3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minpit_n_row\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     \u001b[0mtrain_BP\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 开始训练BP神经网络\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-2b2214cbfc73>\u001b[0m in \u001b[0;36mtrain_BP\u001b[1;34m(X, y, W, B, learn_rate, decay)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mlearn_rate_decay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdecay\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 计算衰减学习率\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mLayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 正向传播算法\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearn_rate_decay\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 反向传播\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[0mcur_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquadratic_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm_y\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 计算当前误差\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-2b2214cbfc73>\u001b[0m in \u001b[0;36moptimizer\u001b[1;34m(Layers, W, B, y, learn_rate)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# 计算每层神经元的误差\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0ml_error_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml_error_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_sigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[0mj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m  \u001b[1;31m# l_delta_arr = [err3, err2, err1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# 倒叙更新优化每层神经元的权重\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\pythontest\\anaconda\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[0mrvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlvals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mrvals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m   1256\u001b[0m                     \u001b[1;34mf\"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: Dot product shape mismatch, (65, 2) vs (1, 15)"
     ]
    }
   ],
   "source": [
    "# \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import boston_housing\n",
    "from keras.layers import Dense, Dropout\n",
    "#from keras.utils import multi_gpu_model\n",
    "from tensorflow.python.keras.utils.multi_gpu_utils import multi_gpu_model\n",
    "\n",
    "from keras import regularizers  # 正则化\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "'''\n",
    "BP神经网络拟合非线性曲线\n",
    "激活函数：sigmoid\n",
    "损失函数：quadratic_cost\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "# 激活函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "def d_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "#  计算损失函数：平方损失函数\n",
    "def quadratic_cost(y_, y):\n",
    "    return np.sum(1 / 2 * np.square(y_ - y))\n",
    "# 归一化数据\n",
    "def normalize(data):\n",
    "    data_min, data_max = data.min(), data.max()\n",
    "    data = (data - data_min) / (data_max - data_min)\n",
    "    return data\n",
    "# 反归一化数据\n",
    "def d_normalize(norm_data, data):\n",
    "    data_min, data_max = data.min(), data.max()\n",
    "    return norm_data * (data_max - data_min) + data_min\n",
    "# 前向传播算法\n",
    "def prediction(l0, W, B):\n",
    "    Layers = [l0]\n",
    "    for i in range(len(W)):\n",
    "        Layers.append(sigmoid(x=Layers[-1].dot(W[i]) + B[i]))\n",
    "    return Layers\n",
    "# 反向传播算法：根据损失函数优化各个隐藏层的权重\n",
    "def optimizer(Layers, W, B, y, learn_rate):\n",
    "    # 计算最后一层误差\n",
    "    l_error_arr = [(y - Layers[-1]) * d_sigmoid(x=Layers[-1])]\n",
    "    # 计算每层神经元的误差\n",
    "    for i in range(len(W) - 1, 0, -1):\n",
    "        l_error_arr.append(l_error_arr[-1].dot(W[i].T) * d_sigmoid(x=Layers[i]))\n",
    "    j = 0  # l_delta_arr = [err3, err2, err1]\n",
    "    # 倒叙更新优化每层神经元的权重\n",
    "    for i in range(len(W) - 1, -1, -1):\n",
    "        W[i] += learn_rate * Layers[i].T.dot(l_error_arr[j])  # W3 += h2 * err3\n",
    "        B[i] += learn_rate * l_error_arr[j]  # B3 += err3\n",
    "        j += 1\n",
    "# 训练BP神经网络\n",
    "def train_BP(X, y, W, B, learn_rate=0.01, decay=0.5):\n",
    "    norm_X, norm_y = normalize(data=X), normalize(data=y)  # 归一化处理\n",
    "    end_loss = 0.068  # 结束训练的最小误差\n",
    "    step_arr, loss_arr = [], []  # 记录单位时刻的误差\n",
    "    step = 1\n",
    "    while True:\n",
    "        learn_rate_decay = learn_rate * 1.0 / (1.0 + decay * step)  # 计算衰减学习率\n",
    "        Layers = prediction(l0=norm_X, W=W, B=B)  # 正向传播算法\n",
    "        optimizer(Layers=Layers, W=W, B=B, y=y, learn_rate=learn_rate_decay)  # 反向传播\n",
    "        cur_loss = quadratic_cost(y_=Layers[-1], y=norm_y)  # 计算当前误差\n",
    "        if step % 1000 == 0:\n",
    "            step_arr.append(step)\n",
    "            loss_arr.append(cur_loss)\n",
    "            print('经过{}次迭代，当前误差为{}'.format(step, cur_loss))\n",
    "        if cur_loss < end_loss:\n",
    "            prediction_ys = d_normalize(norm_data=Layers[-1], data=y)  # 反归一化结果\n",
    "            print('经过{}次迭代，最终误差为：{}'.format(step, cur_loss))\n",
    "            draw_fit_curve(origin_xs=X, origin_ys=y, prediction_ys=prediction_ys, step_arr=step_arr, loss_arr=loss_arr)\n",
    "            break\n",
    "        step += 1\n",
    "# 可视化多项式曲线拟合结果\n",
    "def draw_fit_curve(origin_xs, origin_ys, prediction_ys, step_arr, loss_arr):\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.plot(origin_xs, origin_ys, color='m', linestyle='', marker='.', label='原数据')\n",
    "    ax1.plot(origin_xs, prediction_ys, color='#009688', label='拟合曲线')\n",
    "    plt.title(s='BP神经网络拟合非线性曲线')\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.plot(step_arr, loss_arr, color='red', label='误差曲线')\n",
    "    plt.title(s='BP神经网络误差下降曲线')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "if __name__ == '__main__':\n",
    "    #np.random.seed(1)\n",
    "\n",
    "    x_train = pd.read_excel('dataframe1.xlsx')\n",
    "    y_train = pd.read_excel('location.xlsx',usecols=['x'])\n",
    "    x_valid = pd.read_csv('data_t1.csv')\n",
    "    y_valid = pd.read_excel('weizhi1.xlsx',usecols=['x'])\n",
    "    inpit_n_row = np.shape(x_train)[0]  # 输入层神经元节点行数\n",
    "    input_n_col = np.shape(x_train)[1] # 输入层神经元节点列数\n",
    "    hidden_n_1 = 60  # 第一个隐藏层节点数\n",
    "    hidden_n_2 = 15  # 第二个隐藏层节点数\n",
    "#     x_train_pd = pd.DataFrame(x_train)\n",
    "#     y_train_pd = pd.DataFrame(y_train)\n",
    "#     x_valid_pd = pd.DataFrame(x_valid)\n",
    "#     y_valid_pd = pd.DataFrame(y_valid)\n",
    "    # X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1], [0, 0, 0], [0, 1, 0], [1, 0, 0], [1, 1, 0]])\n",
    "    # y = np.array([[0], [1], [1], [0], [0], [1], [1], [0]])\n",
    "    #X = np.arange(-5, 5, 0.1)[:, np.newaxis]  # 输入层矩阵\n",
    "    X = pd.DataFrame(x_train)\n",
    "    #x_train = pd.read_excel\n",
    "    \n",
    "    #y = 5 * np.sin(X) + 2 * np.random.random()  # 输出层矩阵\n",
    "    y = pd.DataFrame(y_train)\n",
    "    # 第一个隐藏层权重矩阵\n",
    "    W1 = np.random.randn(input_n_col, hidden_n_1) / np.sqrt(inpit_n_row)\n",
    "    b1 = np.zeros((inpit_n_row, hidden_n_1))\n",
    "    # 第二个隐藏层权重矩阵\n",
    "    W2 = np.random.randn(hidden_n_1, hidden_n_2) / np.sqrt(inpit_n_row)\n",
    "    b2 = np.zeros((inpit_n_row, hidden_n_2))\n",
    "    # 输出层权重矩阵\n",
    "    W3 = np.random.randn(hidden_n_2, 1) / np.sqrt(inpit_n_row)\n",
    "    b3 = np.zeros((inpit_n_row, 1))\n",
    "    W, B = [W1, W2, W3], [b1, b2, b3]\n",
    "    train_BP(X=X, y=y, W=W, B=B, learn_rate=0.15, decay=0.5)  # 开始训练BP神经网络\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b165e845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1����ʱ��ns</th>\n",
       "      <th>1����dB</th>\n",
       "      <th>1����</th>\n",
       "      <th>1����.1</th>\n",
       "      <th>1����ʱ��us</th>\n",
       "      <th>2����ʱ��ns</th>\n",
       "      <th>2����dB</th>\n",
       "      <th>2����</th>\n",
       "      <th>2����.1</th>\n",
       "      <th>2����ʱ��us</th>\n",
       "      <th>3����ʱ��ns</th>\n",
       "      <th>3����dB</th>\n",
       "      <th>3����</th>\n",
       "      <th>3����.1</th>\n",
       "      <th>3����ʱ��us</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.267224e+09</td>\n",
       "      <td>90.63</td>\n",
       "      <td>22223.59</td>\n",
       "      <td>424.0</td>\n",
       "      <td>67368.1</td>\n",
       "      <td>9.267221e+09</td>\n",
       "      <td>91.12</td>\n",
       "      <td>22177.94</td>\n",
       "      <td>412.0</td>\n",
       "      <td>72412.5</td>\n",
       "      <td>9.267228e+09</td>\n",
       "      <td>89.24</td>\n",
       "      <td>27050.73</td>\n",
       "      <td>459.0</td>\n",
       "      <td>74573.1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.371039e+10</td>\n",
       "      <td>65.76</td>\n",
       "      <td>496.03</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5144.4</td>\n",
       "      <td>2.365220e+10</td>\n",
       "      <td>90.54</td>\n",
       "      <td>18106.32</td>\n",
       "      <td>392.0</td>\n",
       "      <td>58886.7</td>\n",
       "      <td>2.365224e+10</td>\n",
       "      <td>90.06</td>\n",
       "      <td>21384.01</td>\n",
       "      <td>444.0</td>\n",
       "      <td>65440.7</td>\n",
       "      <td>17.8</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.702677e+10</td>\n",
       "      <td>94.07</td>\n",
       "      <td>31165.93</td>\n",
       "      <td>521.0</td>\n",
       "      <td>70659.0</td>\n",
       "      <td>3.702681e+10</td>\n",
       "      <td>97.93</td>\n",
       "      <td>33403.17</td>\n",
       "      <td>537.0</td>\n",
       "      <td>76202.6</td>\n",
       "      <td>3.702680e+10</td>\n",
       "      <td>99.52</td>\n",
       "      <td>40229.27</td>\n",
       "      <td>607.0</td>\n",
       "      <td>85070.4</td>\n",
       "      <td>17.2</td>\n",
       "      <td>11.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.203974e+10</td>\n",
       "      <td>92.12</td>\n",
       "      <td>28948.69</td>\n",
       "      <td>436.0</td>\n",
       "      <td>71003.4</td>\n",
       "      <td>9.203974e+10</td>\n",
       "      <td>96.30</td>\n",
       "      <td>36941.08</td>\n",
       "      <td>527.0</td>\n",
       "      <td>89747.1</td>\n",
       "      <td>9.203973e+10</td>\n",
       "      <td>94.75</td>\n",
       "      <td>41531.23</td>\n",
       "      <td>543.0</td>\n",
       "      <td>83829.5</td>\n",
       "      <td>18.9</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.081450e+11</td>\n",
       "      <td>90.12</td>\n",
       "      <td>22366.65</td>\n",
       "      <td>434.0</td>\n",
       "      <td>79088.4</td>\n",
       "      <td>1.081450e+11</td>\n",
       "      <td>94.13</td>\n",
       "      <td>27334.51</td>\n",
       "      <td>455.0</td>\n",
       "      <td>76821.2</td>\n",
       "      <td>1.081450e+11</td>\n",
       "      <td>92.94</td>\n",
       "      <td>31625.11</td>\n",
       "      <td>515.0</td>\n",
       "      <td>85667.2</td>\n",
       "      <td>17.8</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1.980494e+10</td>\n",
       "      <td>81.66</td>\n",
       "      <td>1505.84</td>\n",
       "      <td>144.0</td>\n",
       "      <td>6295.6</td>\n",
       "      <td>1.980488e+10</td>\n",
       "      <td>100.00</td>\n",
       "      <td>70425.40</td>\n",
       "      <td>712.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>1.980491e+10</td>\n",
       "      <td>91.57</td>\n",
       "      <td>14398.60</td>\n",
       "      <td>530.0</td>\n",
       "      <td>53146.5</td>\n",
       "      <td>21.6</td>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>3.282296e+10</td>\n",
       "      <td>83.83</td>\n",
       "      <td>1153.41</td>\n",
       "      <td>109.0</td>\n",
       "      <td>6554.4</td>\n",
       "      <td>3.282286e+10</td>\n",
       "      <td>99.78</td>\n",
       "      <td>61258.63</td>\n",
       "      <td>689.0</td>\n",
       "      <td>100037.8</td>\n",
       "      <td>3.282302e+10</td>\n",
       "      <td>88.46</td>\n",
       "      <td>11421.35</td>\n",
       "      <td>459.0</td>\n",
       "      <td>56826.7</td>\n",
       "      <td>23.2</td>\n",
       "      <td>7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>3.923427e+10</td>\n",
       "      <td>77.93</td>\n",
       "      <td>918.38</td>\n",
       "      <td>112.0</td>\n",
       "      <td>5184.4</td>\n",
       "      <td>3.923419e+10</td>\n",
       "      <td>98.64</td>\n",
       "      <td>41771.11</td>\n",
       "      <td>682.0</td>\n",
       "      <td>97710.9</td>\n",
       "      <td>3.923426e+10</td>\n",
       "      <td>87.54</td>\n",
       "      <td>7171.08</td>\n",
       "      <td>389.0</td>\n",
       "      <td>28964.8</td>\n",
       "      <td>26.5</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4.427215e+10</td>\n",
       "      <td>73.11</td>\n",
       "      <td>645.04</td>\n",
       "      <td>72.0</td>\n",
       "      <td>4625.2</td>\n",
       "      <td>4.427197e+10</td>\n",
       "      <td>98.99</td>\n",
       "      <td>46288.35</td>\n",
       "      <td>656.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>4.427219e+10</td>\n",
       "      <td>82.59</td>\n",
       "      <td>6872.87</td>\n",
       "      <td>368.0</td>\n",
       "      <td>43945.0</td>\n",
       "      <td>29.3</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1����ʱ��ns  1����dB     1����  1����.1  1����ʱ��us    2����ʱ��ns  \\\n",
       "0   9.267224e+09    90.63  22223.59    424.0     67368.1  9.267221e+09   \n",
       "1   2.371039e+10    65.76    496.03     13.0      5144.4  2.365220e+10   \n",
       "2   3.702677e+10    94.07  31165.93    521.0     70659.0  3.702681e+10   \n",
       "3   9.203974e+10    92.12  28948.69    436.0     71003.4  9.203974e+10   \n",
       "4   1.081450e+11    90.12  22366.65    434.0     79088.4  1.081450e+11   \n",
       "..           ...      ...       ...      ...         ...           ...   \n",
       "61  1.980494e+10    81.66   1505.84    144.0      6295.6  1.980488e+10   \n",
       "62  3.282296e+10    83.83   1153.41    109.0      6554.4  3.282286e+10   \n",
       "63  3.923427e+10    77.93    918.38    112.0      5184.4  3.923419e+10   \n",
       "64  4.427215e+10    73.11    645.04     72.0      4625.2  4.427197e+10   \n",
       "65           NaN      NaN       NaN      NaN         NaN           NaN   \n",
       "\n",
       "    2����dB     2����  2����.1  2����ʱ��us    3����ʱ��ns  3����dB     3����  \\\n",
       "0     91.12  22177.94    412.0     72412.5  9.267228e+09    89.24  27050.73   \n",
       "1     90.54  18106.32    392.0     58886.7  2.365224e+10    90.06  21384.01   \n",
       "2     97.93  33403.17    537.0     76202.6  3.702680e+10    99.52  40229.27   \n",
       "3     96.30  36941.08    527.0     89747.1  9.203973e+10    94.75  41531.23   \n",
       "4     94.13  27334.51    455.0     76821.2  1.081450e+11    92.94  31625.11   \n",
       "..      ...       ...      ...         ...           ...      ...       ...   \n",
       "61   100.00  70425.40    712.0    100000.0  1.980491e+10    91.57  14398.60   \n",
       "62    99.78  61258.63    689.0    100037.8  3.282302e+10    88.46  11421.35   \n",
       "63    98.64  41771.11    682.0     97710.9  3.923426e+10    87.54   7171.08   \n",
       "64    98.99  46288.35    656.0    100000.0  4.427219e+10    82.59   6872.87   \n",
       "65      NaN       NaN      NaN         NaN           NaN      NaN       NaN   \n",
       "\n",
       "    3����.1  3����ʱ��us     x     y  \n",
       "0     459.0     74573.1  19.0  10.5  \n",
       "1     444.0     65440.7  17.8  10.8  \n",
       "2     607.0     85070.4  17.2  11.2  \n",
       "3     543.0     83829.5  18.9  12.4  \n",
       "4     515.0     85667.2  17.8  12.5  \n",
       "..      ...         ...   ...   ...  \n",
       "61    530.0     53146.5  21.6   7.6  \n",
       "62    459.0     56826.7  23.2   7.3  \n",
       "63    389.0     28964.8  26.5   6.7  \n",
       "64    368.0     43945.0  29.3   6.8  \n",
       "65      NaN         NaN   NaN   NaN  \n",
       "\n",
       "[66 rows x 17 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "train_data_path = 'C:/Users/Cronos/Desktop/code/data/train.csv'\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24df76b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v1' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-27d2ee723913>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;31m# 训练模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         train_model(hidden_floors_num, every_hidden_floor_num, learning_rate, activation, regularization,\n\u001b[0m\u001b[0;32m    288\u001b[0m                     regularization_rate, total_step, train_data_path, model_save_path)\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-27d2ee723913>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(hidden_floors_num, every_hidden_floor_num, learning_rate, activation, regularization, regularization_rate, total_step, train_data_path, model_save_path)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[0minitial_w_and_b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_floors_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[0my_pre\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_real\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m     \u001b[0mtrain_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-27d2ee723913>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(x, y_real)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfloor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_floors_num\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mw_floor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1_regularizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregularization_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_floor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquared_difference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.compat.v1' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf2\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "createVar = locals()\n",
    "\n",
    "'''\n",
    "建立一个网络结构可变的BP神经网络通用代码：\n",
    "在训练时各个参数的意义：\n",
    "hidden_floors_num：隐藏层的个数\n",
    "every_hidden_floor_num：每层隐藏层的神经元个数\n",
    "learning_rate：学习速率\n",
    "activation：激活函数\n",
    "regularization：正则化方式\n",
    "regularization_rate：正则化比率\n",
    "total_step：总的训练次数\n",
    "train_data_path：训练数据路径\n",
    "model_save_path：模型保存路径\n",
    "利用训练好的模型对验证集进行验证时各个参数的意义：\n",
    "model_save_path：模型保存路径\n",
    "validate_data_path：验证集路径\n",
    "precision：精度\n",
    "利用训练好的模型进行预测时各个参数的意义：\n",
    "model_save_path：模型的保存路径\n",
    "predict_data_path：预测数据路径\n",
    "predict_result_save_path：预测结果保存路径\n",
    "'''\n",
    "\n",
    "\n",
    "# 训练模型全局参数\n",
    "hidden_floors_num = 1\n",
    "every_hidden_floor_num = [50]\n",
    "learning_rate = 0.00001\n",
    "activation = 'tanh'\n",
    "regularization = 'L1'\n",
    "regularization_rate = 0.0001\n",
    "total_step = 1000000\n",
    "#train_data_path = 'C:/Users/Cronos/Desktop/code/数据1/train.csv'\n",
    "train_data_path = 'C:/Users/Cronos/Desktop/code/data/train.csv'\n",
    "model_save_path = 'C:/Users/Cronos/Desktop/code/data/model/predict_model'\n",
    "\n",
    "# 利用模型对验证集进行验证返回正确率\n",
    "model_save_path = 'C:/Users/Cronos/Desktop/code/data/model/predict_model'\n",
    "validate_data_path = 'C:/Users/Cronos/Desktop/code/data/valid.csv'\n",
    "precision = 0.5\n",
    "\n",
    "# 利用模型进行预测全局参数\n",
    "model_save_path = 'C:/Users/Cronos/Desktop/code/data/model/predict_model'\n",
    "predict_data_path = 'C:/Users/Cronos/Desktop/code/data/test.csv'\n",
    "predict_result_save_path = 'C:/Users/Cronos/Desktop/code/data/test_predict.csv'\n",
    "\n",
    "\n",
    "def inputs(train_data_path):\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    X = np.array(train_data.iloc[:, :-2])\n",
    "    Y = np.array(train_data.iloc[:, -2:])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def make_hidden_layer(pre_lay_num, cur_lay_num, floor):\n",
    "    createVar['w' + str(floor)] = tf.Variable(tf.random_normal([pre_lay_num, cur_lay_num], stddev=1))\n",
    "    createVar['b' + str(floor)] = tf.Variable(tf.random_normal([cur_lay_num], stddev=1))\n",
    "    return eval('w'+str(floor)), eval('b'+str(floor))\n",
    "\n",
    "\n",
    "def initial_w_and_b(all_floors_num):\n",
    "    # 初始化隐藏层的w, b\n",
    "    for floor in range(2, hidden_floors_num+3):\n",
    "        pre_lay_num = all_floors_num[floor-2]\n",
    "        cur_lay_num = all_floors_num[floor-1]\n",
    "        w_floor, b_floor = make_hidden_layer(pre_lay_num, cur_lay_num, floor)\n",
    "        createVar['w' + str(floor)] = w_floor\n",
    "        createVar['b' + str(floor)] = b_floor\n",
    "\n",
    "\n",
    "def cal_floor_output(x, floor):\n",
    "    w_floor = eval('w'+str(floor))\n",
    "    b_floor = eval('b'+str(floor))\n",
    "    if activation == 'sigmoid':\n",
    "        output = tf.sigmoid(tf.matmul(x, w_floor) + b_floor)\n",
    "    if activation == 'tanh':\n",
    "        output = tf.tanh(tf.matmul(x, w_floor) + b_floor)\n",
    "    if activation == 'relu':\n",
    "        output = tf.nn.relu(tf.matmul(x, w_floor) + b_floor)\n",
    "    return output\n",
    "\n",
    "\n",
    "def inference(x):\n",
    "    output = x\n",
    "    for floor in range(2, hidden_floors_num+2):\n",
    "        output = cal_floor_output(output, floor)\n",
    "\n",
    "    floor = hidden_floors_num+2\n",
    "    w_floor = eval('w'+str(floor))\n",
    "    b_floor = eval('b'+str(floor))\n",
    "    output = tf.matmul(output, w_floor) + b_floor\n",
    "    return output\n",
    "\n",
    "\n",
    "def loss(x, y_real):\n",
    "    y_pre = inference(x)\n",
    "    if regularization == 'None':\n",
    "        total_loss = tf.reduce_sum(tf.squared_difference(y_real, y_pre))\n",
    "\n",
    "    if regularization == 'L1':\n",
    "        total_loss = 0\n",
    "        for floor in range(2, hidden_floors_num + 3):\n",
    "            w_floor = eval('w' + str(floor))\n",
    "            total_loss = total_loss + tf.contrib.layers.l1_regularizer(regularization_rate)(w_floor)\n",
    "        total_loss = total_loss + tf.reduce_sum(tf.squared_difference(y_real, y_pre))\n",
    "\n",
    "    if regularization == 'L2':\n",
    "        total_loss = 0\n",
    "        for floor in range(2, hidden_floors_num + 3):\n",
    "            w_floor = eval('w' + str(floor))\n",
    "            total_loss = total_loss + tf.contrib.layers.l2_regularizer(regularization_rate)(w_floor)\n",
    "        total_loss = total_loss + tf.reduce_sum(tf.squared_difference(y_real, y_pre))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train(total_loss):\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "    return train_op\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "def train_model(hidden_floors_num, every_hidden_floor_num, learning_rate, activation, regularization,\n",
    "                regularization_rate, total_step, train_data_path, model_save_path):\n",
    "    X, Y = inputs(train_data_path)\n",
    "    X_dim = X.shape[1]\n",
    "    all_floors_num = [X_dim] + every_hidden_floor_num + [1]\n",
    "\n",
    "    # 将参数保存到和model_save_path相同的文件夹下， 恢复模型进行预测时加载这些参数创建神经网络\n",
    "    temp = model_save_path.split('/')\n",
    "    model_name = temp[-1]\n",
    "    parameter_path = ''\n",
    "    for i in range(len(temp)-1):\n",
    "        parameter_path = parameter_path + temp[i] + '/'\n",
    "    parameter_path = parameter_path + model_name + '_parameter.txt'\n",
    "    with open(parameter_path, 'w') as f:\n",
    "        f.write(\"all_floors_num:\")\n",
    "        for i in all_floors_num:\n",
    "            f.write(str(i) + ' ')\n",
    "        f.write('\\n')\n",
    "        f.write('activation:')\n",
    "        f.write(str(activation))\n",
    "\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, X_dim])\n",
    "    y_real = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "    initial_w_and_b(all_floors_num)\n",
    "    y_pre = inference(x)\n",
    "    total_loss = loss(x, y_real)\n",
    "    train_op = train(total_loss)\n",
    "\n",
    "    # 记录在训练集上的正确率\n",
    "    train_accuracy = tf.reduce_mean(tf.cast(tf.abs(y_pre - y_real) < precision, tf.float32))\n",
    "\n",
    "    # 保存模型\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # 在一个会话对象中启动数据流图，搭建流程\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for step in range(total_step):\n",
    "        sess.run([train_op], feed_dict={x: X[0:, :], y_real: Y[0:, :]})\n",
    "        if step % 1000 == 0:\n",
    "            saver.save(sess, model_save_path)\n",
    "            total_loss_value = sess.run(total_loss, feed_dict={x: X[0:, :], y_real: Y[0:, :]})\n",
    "            print('train step is ', step, ', total loss value is ', total_loss_value,\n",
    "                  ', train_accuracy', sess.run(train_accuracy, feed_dict={x: X, y_real: Y}),\n",
    "                  ', precision is ', precision)\n",
    "\n",
    "    saver.save(sess, model_save_path)\n",
    "    sess.close()\n",
    "\n",
    "\n",
    "def validate(model_save_path, validate_data_path, precision):\n",
    "    # **********************根据model_save_path推出模型参数路径, 解析出all_floors_num和activation****************\n",
    "    temp = model_save_path.split('/')\n",
    "    model_name = temp[-1]\n",
    "    parameter_path = ''\n",
    "    for i in range(len(temp)-1):\n",
    "        parameter_path = parameter_path + temp[i] + '/'\n",
    "    parameter_path = parameter_path + model_name + '_parameter.txt'\n",
    "    with open(parameter_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # 从读取的内容中解析all_floors_num\n",
    "    temp = lines[0].split(':')[-1].split(' ')\n",
    "    all_floors_num = []\n",
    "    for i in range(len(temp)-1):\n",
    "        all_floors_num = all_floors_num + [int(temp[i])]\n",
    "\n",
    "    # 从读取的内容中解析activation\n",
    "    activation = lines[1].split(':')[-1]\n",
    "    hidden_floors_num = len(all_floors_num) - 2\n",
    "\n",
    "    # **********************读取验证数据*************************************\n",
    "    X, Y = inputs(validate_data_path)\n",
    "    X_dim = X.shape[1]\n",
    "\n",
    "    # **********************创建神经网络************************************\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, X_dim])\n",
    "    y_real = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "    initial_w_and_b(all_floors_num)\n",
    "    y_pre = inference(x)\n",
    "\n",
    "    # 记录在验证集上的正确率\n",
    "    validate_accuracy = tf.reduce_mean(tf.cast(tf.abs(y_pre - y_real) < precision, tf.float32))\n",
    "\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # 读取模型\n",
    "        try:\n",
    "            saver.restore(sess, model_save_path)\n",
    "            print('模型载入成功！')\n",
    "        except:\n",
    "            print('模型不存在，请先训练模型！')\n",
    "            return\n",
    "        validate_accuracy_value = sess.run(validate_accuracy, feed_dict={x: X, y_real: Y})\n",
    "        print('validate_accuracy is ', validate_accuracy_value)\n",
    "\n",
    "    return validate_accuracy_value\n",
    "\n",
    "\n",
    "def predict(model_save_path, predict_data_path, predict_result_save_path):\n",
    "    # **********************根据model_save_path推出模型参数路径, 解析出all_floors_num和activation****************\n",
    "    temp = model_save_path.split('/')\n",
    "    model_name = temp[-1]\n",
    "    parameter_path = ''\n",
    "    for i in range(len(temp)-1):\n",
    "        parameter_path = parameter_path + temp[i] + '/'\n",
    "    parameter_path = parameter_path + model_name + '_parameter.txt'\n",
    "    with open(parameter_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # 从读取的内容中解析all_floors_num\n",
    "    temp = lines[0].split(':')[-1].split(' ')\n",
    "    all_floors_num = []\n",
    "    for i in range(len(temp)-1):\n",
    "        all_floors_num = all_floors_num + [int(temp[i])]\n",
    "\n",
    "    # 从读取的内容中解析activation\n",
    "    activation = lines[1].split(':')[-1]\n",
    "    hidden_floors_num = len(all_floors_num) - 2\n",
    "\n",
    "    # **********************读取预测数据*************************************\n",
    "    predict_data = pd.read_csv(predict_data_path)\n",
    "    X = np.array(predict_data.iloc[:, :])\n",
    "    X_dim = X.shape[1]\n",
    "\n",
    "    # **********************创建神经网络************************************\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, X_dim])\n",
    "    initial_w_and_b(all_floors_num)\n",
    "    y_pre = inference(x)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # 读取模型\n",
    "        try:\n",
    "            saver.restore(sess, model_save_path)\n",
    "            print('模型载入成功！')\n",
    "        except:\n",
    "            print('模型不存在，请先训练模型！')\n",
    "            return\n",
    "        y_pre_value = sess.run(y_pre, feed_dict={x: X[0:, :]})\n",
    "\n",
    "        # 将预测结果写入csv文件\n",
    "        predict_data_columns = list(predict_data.columns) + ['predict']\n",
    "        data = np.column_stack([X, y_pre_value])\n",
    "        result = pd.DataFrame(data, columns=predict_data_columns)\n",
    "        result.to_csv(predict_result_save_path, index=False)\n",
    "        print('预测结果保存在：', predict_result_save_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mode = 'train'\n",
    "\n",
    "    if mode == 'train':\n",
    "        # 训练模型\n",
    "        train_model(hidden_floors_num, every_hidden_floor_num, learning_rate, activation, regularization,\n",
    "                    regularization_rate, total_step, train_data_path, model_save_path)\n",
    "\n",
    "    if mode == 'validate':\n",
    "        # 利用模型对验证集进行正确性测试\n",
    "        validate(model_save_path, validate_data_path, precision)\n",
    "\n",
    "    if mode == 'predict':\n",
    "        #利用模型进行预测\n",
    "        predict(model_save_path, predict_data_path, predict_result_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19faf6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "createVar = locals()\n",
    "\n",
    "'''\n",
    "建立一个网络结构可变的BP神经网络通用代码：\n",
    "在训练时各个参数的意义：\n",
    "hidden_floors_num：隐藏层的个数\n",
    "every_hidden_floor_num：每层隐藏层的神经元个数\n",
    "learning_rate：学习速率\n",
    "activation：激活函数\n",
    "regularization：正则化方式\n",
    "regularization_rate：正则化比率\n",
    "total_step：总的训练次数\n",
    "train_data_path：训练数据路径\n",
    "model_save_path：模型保存路径\n",
    "利用训练好的模型对验证集进行验证时各个参数的意义：\n",
    "model_save_path：模型保存路径\n",
    "validate_data_path：验证集路径\n",
    "precision：精度\n",
    "利用训练好的模型进行预测时各个参数的意义：\n",
    "model_save_path：模型的保存路径\n",
    "predict_data_path：预测数据路径\n",
    "predict_result_save_path：预测结果保存路径\n",
    "'''\n",
    "\n",
    "\n",
    "# 训练模型全局参数\n",
    "hidden_floors_num = 1\n",
    "every_hidden_floor_num = [50]\n",
    "learning_rate = 0.00001\n",
    "activation = 'tanh'\n",
    "regularization = 'L1'\n",
    "regularization_rate = 0.0001\n",
    "total_step = 100000\n",
    "train_data_path = 'train.csv'\n",
    "model_save_path = 'model/predict_model'\n",
    "\n",
    "# 利用模型对验证集进行验证返回正确率\n",
    "model_save_path = 'model/predict_model'\n",
    "validate_data_path = 'validate.csv'\n",
    "precision = 0.5\n",
    "\n",
    "# 利用模型进行预测全局参数\n",
    "model_save_path = 'model/predict_model'\n",
    "predict_data_path = 'test.csv'\n",
    "predict_result_save_path = 'test_predict.csv'\n",
    "\n",
    "\n",
    "def inputs(train_data_path):\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    X = np.array(train_data.iloc[:, :-1])\n",
    "    Y = np.array(train_data.iloc[:, -1:])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def make_hidden_layer(pre_lay_num, cur_lay_num, floor):\n",
    "    createVar['w' + str(floor)] = tf.Variable(tf.random_normal([pre_lay_num, cur_lay_num], stddev=1))\n",
    "    createVar['b' + str(floor)] = tf.Variable(tf.random_normal([cur_lay_num], stddev=1))\n",
    "    return eval('w'+str(floor)), eval('b'+str(floor))\n",
    "\n",
    "\n",
    "def initial_w_and_b(all_floors_num):\n",
    "    # 初始化隐藏层的w, b\n",
    "    for floor in range(2, hidden_floors_num+3):\n",
    "        pre_lay_num = all_floors_num[floor-2]\n",
    "        cur_lay_num = all_floors_num[floor-1]\n",
    "        w_floor, b_floor = make_hidden_layer(pre_lay_num, cur_lay_num, floor)\n",
    "        createVar['w' + str(floor)] = w_floor\n",
    "        createVar['b' + str(floor)] = b_floor\n",
    "\n",
    "\n",
    "def cal_floor_output(x, floor):\n",
    "    w_floor = eval('w'+str(floor))\n",
    "    b_floor = eval('b'+str(floor))\n",
    "    if activation == 'sigmoid':\n",
    "        output = tf.sigmoid(tf.matmul(x, w_floor) + b_floor)\n",
    "    if activation == 'tanh':\n",
    "        output = tf.tanh(tf.matmul(x, w_floor) + b_floor)\n",
    "    if activation == 'relu':\n",
    "        output = tf.nn.relu(tf.matmul(x, w_floor) + b_floor)\n",
    "    return output\n",
    "\n",
    "\n",
    "def inference(x):\n",
    "    output = x\n",
    "    for floor in range(2, hidden_floors_num+2):\n",
    "        output = cal_floor_output(output, floor)\n",
    "\n",
    "    floor = hidden_floors_num+2\n",
    "    w_floor = eval('w'+str(floor))\n",
    "    b_floor = eval('b'+str(floor))\n",
    "    output = tf.matmul(output, w_floor) + b_floor\n",
    "    return output\n",
    "\n",
    "\n",
    "def loss(x, y_real):\n",
    "    y_pre = inference(x)\n",
    "    if regularization == 'None':\n",
    "        total_loss = tf.reduce_sum(tf.squared_difference(y_real, y_pre))\n",
    "\n",
    "    if regularization == 'L1':\n",
    "        total_loss = 0\n",
    "        for floor in range(2, hidden_floors_num + 3):\n",
    "            w_floor = eval('w' + str(floor))\n",
    "            total_loss = total_loss + tf.contrib.layers.l1_regularizer(regularization_rate)(w_floor)\n",
    "        total_loss = total_loss + tf.reduce_sum(tf.squared_difference(y_real, y_pre))\n",
    "\n",
    "    if regularization == 'L2':\n",
    "        total_loss = 0\n",
    "        for floor in range(2, hidden_floors_num + 3):\n",
    "            w_floor = eval('w' + str(floor))\n",
    "            total_loss = total_loss + tf.contrib.layers.l2_regularizer(regularization_rate)(w_floor)\n",
    "        total_loss = total_loss + tf.reduce_sum(tf.squared_difference(y_real, y_pre))\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train(total_loss):\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "    return train_op\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "def train_model(hidden_floors_num, every_hidden_floor_num, learning_rate, activation, regularization,\n",
    "                regularization_rate, total_step, train_data_path, model_save_path):\n",
    "    file_handle = open('acc.txt', mode='w')\n",
    "    X, Y = inputs(train_data_path)\n",
    "    X_dim = X.shape[1]\n",
    "    all_floors_num = [X_dim] + every_hidden_floor_num + [1]\n",
    "\n",
    "    # 将参数保存到和model_save_path相同的文件夹下， 恢复模型进行预测时加载这些参数创建神经网络\n",
    "    temp = model_save_path.split('/')\n",
    "    model_name = temp[-1]\n",
    "    parameter_path = ''\n",
    "    for i in range(len(temp)-1):\n",
    "        parameter_path = parameter_path + temp[i] + '/'\n",
    "    parameter_path = parameter_path + model_name + '_parameter.txt'\n",
    "    with open(parameter_path, 'w') as f:\n",
    "        f.write(\"all_floors_num:\")\n",
    "        for i in all_floors_num:\n",
    "            f.write(str(i) + ' ')\n",
    "        f.write('\\n')\n",
    "        f.write('activation:')\n",
    "        f.write(str(activation))\n",
    "\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, X_dim])\n",
    "    y_real = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "    initial_w_and_b(all_floors_num)\n",
    "    y_pre = inference(x)\n",
    "    total_loss = loss(x, y_real)\n",
    "    train_op = train(total_loss)\n",
    "\n",
    "    # 记录在训练集上的正确率\n",
    "    train_accuracy = tf.reduce_mean(tf.cast(tf.abs(y_pre - y_real) < precision, tf.float32))\n",
    "    print(y_pre)\n",
    "    # 保存模型\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # 在一个会话对象中启动数据流图，搭建流程\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    for step in range(total_step):\n",
    "        sess.run([train_op], feed_dict={x: X[0:, :], y_real: Y[0:, :]})\n",
    "        if step % 1000 == 0:\n",
    "            saver.save(sess, model_save_path)\n",
    "            total_loss_value = sess.run(total_loss, feed_dict={x: X[0:, :], y_real: Y[0:, :]})\n",
    "            lxacc=sess.run(train_accuracy, feed_dict={x: X, y_real: Y})\n",
    "            print('train step is ', step, ', total loss value is ', total_loss_value,\n",
    "                  ', train_accuracy', lxacc,\n",
    "                  ', precision is ', precision)\n",
    "\n",
    "            file_handle.write(str(lxacc)+\"\\n\")\n",
    "\n",
    "\n",
    "    saver.save(sess, model_save_path)\n",
    "    sess.close()\n",
    "\n",
    "\n",
    "def validate(model_save_path, validate_data_path, precision):\n",
    "    # **********************根据model_save_path推出模型参数路径, 解析出all_floors_num和activation****************\n",
    "    temp = model_save_path.split('/')\n",
    "    model_name = temp[-1]\n",
    "    parameter_path = ''\n",
    "    for i in range(len(temp)-1):\n",
    "        parameter_path = parameter_path + temp[i] + '/'\n",
    "    parameter_path = parameter_path + model_name + '_parameter.txt'\n",
    "    with open(parameter_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # 从读取的内容中解析all_floors_num\n",
    "    temp = lines[0].split(':')[-1].split(' ')\n",
    "    all_floors_num = []\n",
    "    for i in range(len(temp)-1):\n",
    "        all_floors_num = all_floors_num + [int(temp[i])]\n",
    "\n",
    "    # 从读取的内容中解析activation\n",
    "    activation = lines[1].split(':')[-1]\n",
    "    hidden_floors_num = len(all_floors_num) - 2\n",
    "\n",
    "    # **********************读取验证数据*************************************\n",
    "    X, Y = inputs(validate_data_path)\n",
    "    X_dim = X.shape[1]\n",
    "\n",
    "    # **********************创建神经网络************************************\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, X_dim])\n",
    "    y_real = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "    initial_w_and_b(all_floors_num)\n",
    "    y_pre = inference(x)\n",
    "\n",
    "    # 记录在验证集上的正确率\n",
    "    validate_accuracy = tf.reduce_mean(tf.cast(tf.abs(y_pre - y_real) < precision, tf.float32))\n",
    "\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # 读取模型\n",
    "        try:\n",
    "            saver.restore(sess, model_save_path)\n",
    "            print('模型载入成功！')\n",
    "        except:\n",
    "            print('模型不存在，请先训练模型！')\n",
    "            return\n",
    "        validate_accuracy_value = sess.run(validate_accuracy, feed_dict={x: X, y_real: Y})\n",
    "        print('validate_accuracy is ', validate_accuracy_value)\n",
    "\n",
    "    return validate_accuracy_value\n",
    "\n",
    "\n",
    "def predict(model_save_path, predict_data_path, predict_result_save_path):\n",
    "    # **********************根据model_save_path推出模型参数路径, 解析出all_floors_num和activation****************\n",
    "    temp = model_save_path.split('/')\n",
    "    model_name = temp[-1]\n",
    "    parameter_path = ''\n",
    "    for i in range(len(temp)-1):\n",
    "        parameter_path = parameter_path + temp[i] + '/'\n",
    "    parameter_path = parameter_path + model_name + '_parameter.txt'\n",
    "    with open(parameter_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # 从读取的内容中解析all_floors_num\n",
    "    temp = lines[0].split(':')[-1].split(' ')\n",
    "    all_floors_num = []\n",
    "    for i in range(len(temp)-1):\n",
    "        all_floors_num = all_floors_num + [int(temp[i])]\n",
    "\n",
    "    # 从读取的内容中解析activation\n",
    "    activation = lines[1].split(':')[-1]\n",
    "    hidden_floors_num = len(all_floors_num) - 2\n",
    "\n",
    "    # **********************读取预测数据*************************************\n",
    "    predict_data = pd.read_csv(predict_data_path)\n",
    "    X = np.array(predict_data.iloc[:, :])\n",
    "    X_dim = X.shape[1]\n",
    "\n",
    "    # **********************创建神经网络************************************\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, X_dim])\n",
    "    initial_w_and_b(all_floors_num)\n",
    "    y_pre = inference(x)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # 读取模型\n",
    "        try:\n",
    "            saver.restore(sess, model_save_path)\n",
    "            print('模型载入成功！')\n",
    "        except:\n",
    "            print('模型不存在，请先训练模型！')\n",
    "            return\n",
    "        y_pre_value = sess.run(y_pre, feed_dict={x: X[0:, :]})\n",
    "\n",
    "        # 将预测结果写入csv文件\n",
    "        predict_data_columns = list(predict_data.columns) + ['predict']\n",
    "        data = np.column_stack([X, y_pre_value])\n",
    "        result = pd.DataFrame(data, columns=predict_data_columns)\n",
    "        result.to_csv(predict_result_save_path, index=False)\n",
    "        print('预测结果保存在：', predict_result_save_path)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mode = \"predict\"\n",
    "\n",
    "    if mode == 'train':\n",
    "        # 训练模型\n",
    "        train_model(hidden_floors_num, every_hidden_floor_num, learning_rate, activation, regularization,\n",
    "                    regularization_rate, total_step, train_data_path, model_save_path)\n",
    "\n",
    "    if mode == 'validate':\n",
    "        # 利用模型对验证集进行正确性测试\n",
    "        validate(model_save_path, validate_data_path, precision)\n",
    "\n",
    "    if mode == 'predict':\n",
    "        # 利用模型进行预测\n",
    "        predict(model_save_path, predict_data_path, predict_result_save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
